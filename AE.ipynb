{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "# 指定第一块GPU可用 \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys\n",
    "\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "\n",
    "def load_data(dataset_str):\n",
    "    \"\"\"\n",
    "    Loads input data from gcn/data directory\n",
    "\n",
    "    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n",
    "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n",
    "        object;\n",
    "    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
    "\n",
    "    All objects above must be saved using python pickle module.\n",
    "\n",
    "    :param dataset_str: Dataset name\n",
    "    :return: All data input files loaded (as well the training/test data).\n",
    "    \"\"\"\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\"data/ind.{}.test.index\".format(dataset_str))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "#     print(tx.shape)\n",
    "#     print(len(ty))\n",
    "    if dataset_str == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    idx_train = range(len(y))\n",
    "#     print(test_idx_range)\n",
    "#     print(len(test_idx_range))\n",
    "#     print(x.shape)\n",
    "#     print(len(y))\n",
    "#     print(allx.shape)\n",
    "#     print(len(ally))\n",
    "#     print(tx.shape)\n",
    "#     print(len(ty))\n",
    "    idx_val = range(len(y), len(y)+500)\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, labels\n",
    "\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return sparse_to_tuple(features)\n",
    "\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "\n",
    "def chebyshev_polynomials(adj, k):\n",
    "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n",
    "    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n",
    "\n",
    "    adj_normalized = normalize_adj(adj)\n",
    "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n",
    "    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n",
    "    scaled_laplacian = (2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n",
    "\n",
    "    t_k = list()\n",
    "    t_k.append(sp.eye(adj.shape[0]))\n",
    "    t_k.append(scaled_laplacian)\n",
    "\n",
    "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n",
    "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n",
    "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n",
    "\n",
    "    for i in range(2, k+1):\n",
    "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n",
    "\n",
    "    return sparse_to_tuple(t_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = load_data('pubmed')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "nx.draw(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = load_data('citeseer')[0].toarray().tolist()\n",
    "count = 0\n",
    "for i in a:\n",
    "    \n",
    "    if 1 in i:\n",
    "        count += 1\n",
    "        print(i.index(1))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense  \n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model  \n",
    "from keras.datasets import mnist  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "# import pandas as pd\n",
    "import matplotlib as mpl\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from numpy import *\n",
    "data = np.load('/home/lifuzhen/Dataset/mnist.npz')\n",
    "x_train, x_test = data['x_train'], data['x_test']\n",
    "  \n",
    "x_train = x_train.astype('float32') / 255.  \n",
    "x_test = x_test.astype('float32') / 255.  \n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))  \n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))  \n",
    "print(x_train.shape)  \n",
    "print(x_test.shape)  \n",
    "  \n",
    "encoding_dim = 256  \n",
    "input_img = Input(shape=(784,))  \n",
    "  \n",
    "encoded = Dense(encoding_dim, activation='sigmoid',use_bias=False) \n",
    "encoded1 = encoded(input_img)\n",
    "encoded1 = BatchNormalization(axis=-1)(encoded1)\n",
    "\n",
    "# decoded = Lambda(lambda x: K.dot(x,K.variable(np.transpose(encoded.get_weights()[0]),dtype='float32')))(encoded1)\n",
    "decoded = Dense(784, activation='sigmoid',use_bias=False)\n",
    "decoded1 = decoded(encoded1)  \n",
    "  \n",
    "autoencoder = Model(inputs=input_img, outputs=decoded1)  \n",
    "encoder = Model(inputs=input_img, outputs=encoded1)  \n",
    "  \n",
    "encoded_input = Input(shape=(encoding_dim,))  \n",
    "decoder_layer = autoencoder.layers[-1]  \n",
    "  \n",
    "decoder = Model(inputs=encoded_input, outputs=decoder_layer(encoded_input))  \n",
    "  \n",
    "autoencoder.compile(optimizer='Adam', loss='binary_crossentropy')  \n",
    "  \n",
    "autoencoder.fit(x_train, x_train, epochs=10, batch_size=256,   \n",
    "                shuffle=True, validation_data=(x_test, x_test))  \n",
    "  \n",
    "encoded_imgs = encoder.predict(x_test)  \n",
    "decoded_imgs = decoder.predict(encoded_imgs)  \n",
    "decoded_imgs = autoencoder.predict(x_test)  \n",
    "n = 10  # how many digits we will display  \n",
    "plt.figure(figsize=(20, 4))  \n",
    "for i in range(n):  \n",
    "    ax = plt.subplot(2, n, i + 1)  \n",
    "    plt.imshow(x_test[i].reshape(28, 28))  \n",
    "    plt.gray()  \n",
    "    ax.get_xaxis().set_visible(False)  \n",
    "    ax.get_yaxis().set_visible(False)  \n",
    "  \n",
    "    ax = plt.subplot(2, n, i + 1 + n)  \n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))  \n",
    "    plt.gray()  \n",
    "    ax.get_xaxis().set_visible(False)  \n",
    "    ax.get_yaxis().set_visible(False)  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense  ,Dot ,Lambda\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model  \n",
    "from keras.datasets import mnist  \n",
    "from keras import backend as K\n",
    "import numpy as np  \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt  \n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "# 指定第一块GPU可用 \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)\n",
    "# adj = load_data(dataset)[0]\n",
    "# adj_normalize = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "# feature = load_data(dataset)[1]\n",
    "# # original_train, original_test = feature[load_data(dataset)[5]], feature[load_data(dataset)[7]]\n",
    "# x_train, x_test = adj_normalize.dot(adj_normalize).dot(feature)[load_data(dataset)[5]], adj_normalize.dot(adj_normalize).dot(feature)[load_data(dataset)[7]]\n",
    "# y_train = load_data(dataset)[2][load_data(dataset)[5]].tolist()\n",
    "# sample_num = 4 ##随机采样数\n",
    "# margin = 10  ##类间距\n",
    "########一层#############\n",
    "# encoding_dim = 1024\n",
    "# input_node = Input(shape=(3703,))  \n",
    "# a_input_node = Input(shape=(3703,)) ##anchor\n",
    "# p_input_node = Input(shape=(3703,))  ##positive\n",
    "# n_input_node = Input(shape=(3703,))  ##negative\n",
    "\n",
    "# encoded = Dense(encoding_dim, activation='sigmoid')(input_node)  \n",
    "# decoded = Dense(3703, activation='sigmoid')(encoded)  \n",
    "\n",
    "# autoencoder = Model(inputs=input_node, outputs=decoded)  \n",
    "# encoder = Model(inputs=input_node, outputs=encoded)    \n",
    "\n",
    "# encoded_input = Input(shape=(encoding_dim,))  \n",
    "# decoder_layer = autoencoder.layers[-1]  \n",
    "# decoder = Model(inputs=encoded_input, outputs=decoder_layer(encoded_input)) \n",
    "# a_encoded = autoencoder.layers[-2](a_input_node)\n",
    "# p_encoded = autoencoder.layers[-2](p_input_node)\n",
    "# n_encoded = autoencoder.layers[-2](n_input_node)\n",
    " \n",
    "# triplet = Lambda(lambda x: K.maximum(K.sum(K.square(x[0]-x[1])) - K.sum(K.square(x[0]-x[2]))+ margin,0))([a_encoded,p_encoded,n_encoded])\n",
    "# triplet_model = Model(inputs= [a_input_node,p_input_node,n_input_node], outputs= triplet)\n",
    "# triplet_model.compile(optimizer='Adam', loss=lambda y_true,y_pred: y_pred)\n",
    "# autoencoder.compile(optimizer='Adam', loss='binary_crossentropy')\n",
    "\n",
    "#########两层###################\n",
    "# encoding_dim_1 = 1024\n",
    "# encoding_dim_2 = 1024\n",
    "# input_node = Input(shape=(3703,))  \n",
    "# a_input_node = Input(shape=(3703,)) ##anchor\n",
    "# p_input_node = Input(shape=(3703,))  ##positive\n",
    "# n_input_node = Input(shape=(3703,))  ##negative\n",
    "\n",
    "# encoded_1 = Dense(encoding_dim_1, activation='sigmoid')(input_node)  \n",
    "# encoded_2 = Dense(encoding_dim_2, activation='relu')(encoded_1)  \n",
    "# decoded = Dense(3703, activation='sigmoid')(encoded_2)  \n",
    "\n",
    "# autoencoder = Model(inputs=input_node, outputs=decoded)  \n",
    "# encoder = Model(inputs=input_node, outputs=encoded_2)    \n",
    "\n",
    "# encoded_input = Input(shape=(encoding_dim_2,))  \n",
    "# decoder_layer = autoencoder.layers[-1]  \n",
    "# decoder = Model(inputs=encoded_input, outputs=decoder_layer(encoded_input)) \n",
    "# a_encoded = autoencoder.layers[-2](autoencoder.layers[-3](a_input_node))\n",
    "# p_encoded = autoencoder.layers[-2](autoencoder.layers[-3](p_input_node))\n",
    "# n_encoded = autoencoder.layers[-2](autoencoder.layers[-3](n_input_node))\n",
    " \n",
    "# triplet = Lambda(lambda x: K.maximum(K.sum(K.square(x[0]-x[1])) - K.sum(K.square(x[0]-x[2]))+ margin,0))([a_encoded,p_encoded,n_encoded])\n",
    "# triplet_model = Model(inputs= [a_input_node,p_input_node,n_input_node], outputs= triplet)\n",
    "# triplet_model.compile(optimizer='Adam', loss=lambda y_true,y_pred: y_pred)\n",
    "# autoencoder.compile(optimizer='Adam', loss='binary_crossentropy')\n",
    "\n",
    "############# 两层：A(AXW)W #################\n",
    "\n",
    "# def embedding(x):\n",
    "#     x_new = []\n",
    "#     for i in range(len(x)):\n",
    "#         x_new.append(encoder_1.predict(x[i]))\n",
    "#     sp.vstack(x_new)\n",
    "#     return x_new\n",
    "dataset = 'cora'  #citeseer   cora  pubmed\n",
    "####D^(-0.5)(A+I)D^(-0.5)\n",
    "# adj = load_data(dataset)[0]\n",
    "# adj_normalize = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "# adj_normalize_tf = tf.convert_to_tensor(adj_normalize.toarray(),dtype='float32')\n",
    "######(I - D^(-0.5)AD^(-0.5))^(-1)\n",
    "adj = load_data(dataset)[0]\n",
    "adj_normalize = normalize_adj(adj)\n",
    "adj_normalize = np.linalg.inv(np.eye(adj.shape[0]) - 0.99*adj_normalize.toarray())\n",
    "adj_normalize_tf = tf.convert_to_tensor(adj_normalize,dtype='float32')\n",
    "feature = load_data(dataset)[1]\n",
    "# original_train, original_test = feature[load_data(dataset)[5]], feature[load_data(dataset)[7]]\n",
    "# x_train, x_test = feature[load_data(dataset)[5]], feature[load_data(dataset)[7]]\n",
    "y_train = load_data(dataset)[2][load_data(dataset)[5]].tolist()\n",
    "\n",
    "\n",
    "########存每一类的索引#######\n",
    "C = []\n",
    "for i in range(load_data(dataset)[2].shape[1]):\n",
    "    C.append([])\n",
    "    \n",
    "for j in range(0,load_data(dataset)[2].shape[1]):\n",
    "   \n",
    "    count = 0\n",
    "    for i in y_train:\n",
    "        if i.index(1) == j:\n",
    "            C[j].append(count)\n",
    "        count += 1\n",
    "        \n",
    "        \n",
    "sample_num = 4 ##随机采样数\n",
    "margin = 20##类间距 citeseer 50 cora 10 pubmed\n",
    "####citeseer########\n",
    "# encoding_dim_1 = 2048\n",
    "# encoding_dim_2 = 1024\n",
    "####cora #######\n",
    "encoding_dim_1 = 1024\n",
    "encoding_dim_2 = 1024\n",
    "######pubmed#####\n",
    "# encoding_dim_1 = 8\n",
    "# encoding_dim_2 = 8\n",
    "\n",
    "order_0 = Input(shape=(feature.shape[1],)) \n",
    "\n",
    "def graph_convex(x):\n",
    "    x1 = K.dot(adj_normalize_tf,x)\n",
    "    return x1\n",
    "\n",
    "\n",
    "order_1 = Lambda(function=graph_convex)(order_0)\n",
    "encoded_1 = Dense(encoding_dim_1, activation='relu')(order_1)  \n",
    "# encoder_1 = Model(inputs=input_node,outputs= encoded_1)\n",
    "\n",
    "order_2 = Lambda(function= graph_convex)(encoded_1)\n",
    "encoded_2 = Dense(encoding_dim_2, activation='relu')(order_2)  \n",
    "\n",
    "decoded = Dense(feature.shape[1], activation='sigmoid')(encoded_2)  \n",
    "\n",
    "autoencoder = Model(inputs=order_0, outputs=decoded)  \n",
    "encoder = Model(inputs=order_0, outputs=encoded_2)    \n",
    "\n",
    "encoded_input = Input(shape=(encoding_dim_2,))  \n",
    "decoder_layer = autoencoder.layers[-1]  \n",
    "decoder = Model(inputs=encoded_input, outputs=decoder_layer(encoded_input)) \n",
    "encoded_train = encoder(order_0)\n",
    "\n",
    "# def cluster_metric(x):\n",
    "#     L = K.variable(0,dtype='float32')\n",
    "# #     print(x)\n",
    "#     for i in range(6):\n",
    "#         for j in range(i+1,6):\n",
    "# #             print(center_j)\n",
    "# #             print(D_center)\n",
    "#             L = tf.subtract(L , K.sum(K.square(K.mean(K.gather(x,C[i]),0)- K.mean(K.gather(x,C[j]),0))))\n",
    "# #             print(inter_sum)\n",
    "\n",
    "#         for j in C[i]:\n",
    "#             L = tf.add(L , K.sum(K.square(K.gather(x,j) - K.mean(K.gather(x,C[i]),0))))\n",
    "# #             print(D_sample_center)\n",
    "#     return  L\n",
    "\n",
    "# def cluster_metric(x):\n",
    "#     inter_sum = K.variable(0,dtype='float32')\n",
    "#     intra_sum = K.variable(0,dtype='float32')\n",
    "# #     print(x)\n",
    "#     for i in range(6):\n",
    "#         center_i = K.mean(K.gather(x,C[i]),0)\n",
    "#         for j in range(i+1,6):\n",
    "#             center_j = K.mean(K.gather(x,C[j]),0)\n",
    "# #             print(center_j)\n",
    "#             D_center = K.sum(K.square(center_i-center_j))\n",
    "# #             print(D_center)\n",
    "#             inter_sum = inter_sum + D_center\n",
    "# #             print(inter_sum)\n",
    "\n",
    "#         for j in C[i]:\n",
    "#             D_sample_center = K.sum(K.square(K.gather(x,j) - center_i))\n",
    "# #             print(D_sample_center)\n",
    "#             intra_sum = intra_sum + D_sample_center\n",
    "#     L = K.relu(0.05 * intra_sum - 0.95 * inter_sum + margin)\n",
    "#     return  L\n",
    "# def cluster_metric(x):\n",
    "#     L = K.variable(0,dtype='float32')\n",
    "#     L_intra = K.variable(0,dtype='float32')\n",
    "#     L_inter = K.variable(0,dtype='float32')\n",
    "# #     print(x)\n",
    "#     for i in range(6):\n",
    "#         for m in C[i]:\n",
    "#             L_intra = L_intra + K.mean(K.log(1+K.sum(K.square(K.gather(x,m)-K.gather(x,C[i])),1)))/6.0\n",
    "# #             print(L_intra)\n",
    "#             for j in range(i+1,6):\n",
    "#                 L_inter = L_inter + K.mean(K.log(1+K.sum(K.square(K.gather(x,m)-K.gather(x,C[j])),1)))/15.0\n",
    "#     L = L_intra - L_inter\n",
    "                \n",
    "                \n",
    "\n",
    "#     return  L\n",
    "\n",
    "def cluster_metric(x):\n",
    "    L = K.variable(0,dtype='float32')\n",
    "    L_intra = K.variable(0,dtype='float32')\n",
    "    L_inter = K.variable(0,dtype='float32')\n",
    "#     print(x)\n",
    "    for i in range(load_data(dataset)[2].shape[1]):\n",
    "        for m in C[i]:\n",
    "            L_intra = L_intra + K.mean(K.relu(K.sum(K.square(K.gather(x,m)-K.gather(x,C[i])),1) - margin ))\n",
    "#             print(L_intra)\n",
    "            for j in range(i+1,load_data(dataset)[2].shape[1]):\n",
    "                L_inter = L_inter + K.mean(K.relu( margin - K.sum(K.square(K.gather(x,m)-K.gather(x,C[j])),1)))\n",
    "    L = L_intra + L_inter\n",
    "                \n",
    "                \n",
    "\n",
    "    return  L\n",
    "    \n",
    "Loss = Lambda(function=cluster_metric)(encoded_train)\n",
    "triplet_model = Model(inputs= order_0, outputs= Loss)\n",
    "triplet_model.compile(optimizer='Adam', loss=lambda y_true,y_pred: y_pred)\n",
    "autoencoder.compile(optimizer='Adam', loss='binary_crossentropy')\n",
    "# C0 = []\n",
    "# C1 = []  ##target and positive\n",
    "# C2 = []  ##negative\n",
    "# C3 = []\n",
    "# C4 = []\n",
    "# C5 = []\n",
    "# C = [C0,C1,C2,C3,C4,C5]\n",
    "# for j in range(0,6):\n",
    "   \n",
    "#     count = 0\n",
    "#     for i in y_train:\n",
    "#         if i.index(1) == j:\n",
    "#             C[j].append(x_train[count])\n",
    "#         count += 1\n",
    "# for m in tqdm(range(0,10)):\n",
    "#     for j in range(0,6):\n",
    "# #         C1 = []  ##target and positive\n",
    "# #         C2 = []  ##negative\n",
    "# #         C = [C1,C2]\n",
    "# #         count = 0\n",
    "# #         for i in y_train:\n",
    "# #             if i.index(1) == j:\n",
    "# #                 C1.append(x_train[count])\n",
    "# #             if i.index(1) != j:\n",
    "# #                 C2.append(x_train[count])\n",
    "# #             count += 1\n",
    "# #         print(x_train.shape)  \n",
    "# #         print(x_test.shape)  \n",
    "#         autoencoder.fit(x_train, x_train, epochs=10, batch_size=32,   \n",
    "#                         shuffle=True, validation_data=(x_test, x_test))\n",
    "#         for k in range(6):\n",
    "#             if j != k:\n",
    "#                 a = sp.vstack(sample(C[j],sample_num))\n",
    "#                 p = sp.vstack(sample(C[j],sample_num))\n",
    "#                 n = sp.vstack(sample(C[k],sample_num))\n",
    "# #                 triplet_model.fit([sp.vstack(sample(C[j],sample_num)),sp.vstack(sample(C[j],sample_num)),sp.vstack(sample(C[k],sample_num))],\n",
    "# #                               np.zeros((sample_num,1)), epochs=20, batch_size=2,shuffle=True)\n",
    "#                 triplet_model.fit([a,p,n],\n",
    "#                               np.zeros((sample_num,1)), epochs=20, batch_size=2,shuffle=True)\n",
    "\n",
    "for m in tqdm(range(0,30)):\n",
    "#         C1 = []  ##target and positive\n",
    "#         C2 = []  ##negative\n",
    "#         C = [C1,C2]\n",
    "#         count = 0\n",
    "#         for i in y_train:\n",
    "#             if i.index(1) == j:\n",
    "#                 C1.append(x_train[count])\n",
    "#             if i.index(1) != j:\n",
    "#                 C2.append(x_train[count])\n",
    "#             count += 1\n",
    "#         print(x_train.shape)  \n",
    "#         print(x_test.shape)  \n",
    "        autoencoder.fit(feature, feature, epochs=10,batch_size=feature.shape[0],   \n",
    "                        shuffle=False)\n",
    "#                 a = sp.vstack(sample(C[j],sample_num))\n",
    "#                 p = sp.vstack(sample(C[j],sample_num))\n",
    "#                 n = sp.vstack(sample(C[k],sample_num))\n",
    "#                 triplet_model.fit([sp.vstack(sample(C[j],sample_num)),sp.vstack(sample(C[j],sample_num)),sp.vstack(sample(C[k],sample_num))],\n",
    "#                               np.zeros((sample_num,1)), epochs=20, batch_size=2,shuffle=True)\n",
    "        triplet_model.fit(feature,np.zeros((feature.shape[0],1)),batch_size=feature.shape[0], epochs=50,shuffle=False)\n",
    "#         print(triplet_model.predict(feature,batch_size=3327))\n",
    "\n",
    "\n",
    "          \n",
    "\n",
    "encoded_node = encoder.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "proj = TSNE(random_state=123).fit_transform(encoded_node)\n",
    "import seaborn as sns\n",
    "import matplotlib.patheffects as PathEffects\n",
    "y = []\n",
    "hot_y = load_data(dataset)[4][load_data(dataset)[7]].tolist()\n",
    "for i in hot_y:\n",
    "    y.append(i.index(1))\n",
    "y = np.array(y)\n",
    "def scatter(x, colors):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", load_data(dataset)[4].shape[1]))\n",
    "    print(palette)\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "scatter(proj, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 支持向量机+GBDT+RF+KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import scipy.io as sio\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "dataset = 'pubmed'\n",
    "SDNE = sio.loadmat(\"/home/lifuzhen/SDNE/result/pubmed-Thu-Nov--1-16:33:33-2018/embedding.mat\")['embedding']\n",
    "trainX , trainY = SDNE[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "testX, testY = SDNE[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "SVC = SVC(C=0.5,kernel='rbf')\n",
    "GBDT = GradientBoostingClassifier(n_estimators=8)\n",
    "RF = RandomForestClassifier()\n",
    "KNN = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "SVC.fit(trainX, trainY)\n",
    "GBDT.fit(trainX, trainY)\n",
    "RF.fit(trainX, trainY)\n",
    "KNN.fit(trainX,trainY)\n",
    "\n",
    "pred_SVC = SVC.predict(testX)\n",
    "pred_GBDT = GBDT.predict(testX)\n",
    "pred_RF = RF.predict(testX)\n",
    "pred_KNN = KNN.predict(testX)\n",
    "\n",
    "auc_SVC = metrics.accuracy_score(testY , pred_SVC)\n",
    "auc_GBDT = metrics.accuracy_score(testY , pred_GBDT)\n",
    "auc_RF = metrics.accuracy_score(testY , pred_RF)\n",
    "auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "\n",
    "print('SVC_auc：%f'%auc_SVC)\n",
    "print('GBDT_auc：%f'%auc_GBDT)\n",
    "print('RF_auc：%f'%auc_RF)\n",
    "print('KNN_auc:%f'%auc_KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "\n",
    "def scatter(x, colors):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", load_data(dataset)[4].shape[1]))\n",
    "#     print(palette)\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "acc = []\n",
    "for n in range(1):\n",
    "    dataset = 'pubmed'  #citeseer   cora  pubmed\n",
    "    adj = load_data(dataset)[0]\n",
    "    adj_normalize = normalize_adj(adj + sp.eye(adj.shape[0])).toarray()\n",
    "\n",
    "    adj = load_data(dataset)[0]\n",
    "    adj_normalize = normalize_adj(adj)\n",
    "    adj_normalize = np.linalg.inv(np.eye(adj.shape[0]) - 0.95*adj_normalize.toarray())\n",
    "    feature = load_data(dataset)[1].toarray()\n",
    "\n",
    "    trainX , trainY = np.dot(adj_normalize, feature)[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "    testX, testY = np.dot(adj_normalize, feature)[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "    svc = SVC(C=3,kernel='rbf')#cora 1.7 citeseer 3\n",
    "    gbdt = GradientBoostingClassifier(n_estimators=8)\n",
    "    rf = RandomForestClassifier()\n",
    "    knn = KNeighborsClassifier()\n",
    "\n",
    "    svc.fit(trainX, trainY)\n",
    "    gbdt.fit(trainX, trainY)\n",
    "    rf.fit(trainX, trainY)\n",
    "    knn.fit(trainX,trainY)\n",
    "\n",
    "    pred_SVC = svc.predict(testX)\n",
    "    pred_GBDT = gbdt.predict(testX)\n",
    "    pred_RF = rf.predict(testX)\n",
    "    pred_KNN = knn.predict(testX)\n",
    "\n",
    "    auc_SVC = metrics.accuracy_score(testY , pred_SVC)\n",
    "    auc_GBDT = metrics.accuracy_score(testY , pred_GBDT)\n",
    "    auc_RF = metrics.accuracy_score(testY , pred_RF)\n",
    "    auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "    acc.append(auc_KNN)\n",
    "    \n",
    "    print('SVC_auc：%f'%auc_SVC)\n",
    "    print('GBDT_auc：%f'%auc_GBDT)\n",
    "    print('RF_auc：%f'%auc_RF)\n",
    "    print('KNN_auc:%f'%auc_KNN)\n",
    "    proj1 = TSNE(random_state=123).fit_transform(np.dot(adj_normalize, feature))\n",
    "\n",
    "    y = onehot2index(load_data(dataset)[8])\n",
    "    scatter(proj1, y)\n",
    "    plt.show()\n",
    "print('mean:%f'%np.mean(acc))\n",
    "print('std:%f'%np.std(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data(dataset)[8].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 迹约束对比实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense  ,Dot ,Lambda, Dropout, BatchNormalization\n",
    "from keras.models import Model  \n",
    "from keras.datasets import mnist  \n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "import keras\n",
    "import numpy as np  \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt  \n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import matplotlib.patheffects as PathEffects\n",
    "K.clear_session()\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "\n",
    "def scatter(x, colors):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", load_data(dataset)[4].shape[1]))\n",
    "#     print(palette)\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "# 指定第一块GPU可用 \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)\n",
    "\n",
    "dataset = 'pubmed'  #citeseer   cora  pubmed\n",
    "acc = []\n",
    "for reg in range(5):\n",
    "    maxi = 0\n",
    "    feature = load_data(dataset)[1].toarray()\n",
    "    ####自带邻接矩阵\n",
    "    adj = load_data(dataset)[0]\n",
    "    adj_normalize = np.diag(np.sum(adj.toarray(), axis= -1)) - adj.toarray()\n",
    "    adj_normalize_tf = tf.convert_to_tensor(adj_normalize,dtype='float32')\n",
    "    # #####逼近式\n",
    "    # alpha = 0.9  ###局部强度\n",
    "    # adj = load_data(dataset)[0]\n",
    "    # adj = adj.astype('float32')\n",
    "    # adj_normalize = normalize_adj(adj)\n",
    "    # adj_normalize = np.linalg.inv(np.eye(adj.shape[0]) - alpha*adj_normalize.toarray())\n",
    "    # # adj_normalize = np.eye(adj.shape[0]) + alpha*adj_normalize + alpha**2*np.dot(adj_normalize,adj_normalize.T)\n",
    "    # adj_normalize_tf = tf.convert_to_tensor(adj_normalize,dtype='float32')\n",
    "\n",
    "    encoding_dim_1 = 256\n",
    "    e = 5e-4 ###cora: 3e-2 citeseer:8e-2  pubmed: 5e-4\n",
    "    t = 1.0  ##高斯核系数\n",
    "    epoch = 500 ###cora: 300 citeseer:250  pubmed:500\n",
    "    time = []\n",
    "#     acc = []\n",
    "    # rate = 0.1  ###相似度矩阵学习率\n",
    "    for n in tqdm(range(1)):\n",
    "        input_1 = Input(shape=(feature.shape[1],)) \n",
    "\n",
    "        encoded = Dense(encoding_dim_1, activation='sigmoid', use_bias=True)(input_1)  \n",
    "        encoded1 = BatchNormalization(axis = -1)(encoded)\n",
    "\n",
    "        decoded = Dense(feature.shape[1], activation='sigmoid', use_bias=True)(encoded1)\n",
    "    #     decoded = BatchNormalization(axis = -1)(decoded)\n",
    "\n",
    "        autoencoder = Model(inputs=input_1, outputs=decoded)  \n",
    "        encoder = Model(inputs=input_1, outputs=encoded)    \n",
    "\n",
    "        def myloss(y_true, y_pred):\n",
    "            recon = K.mean(K.square(y_true - y_pred), axis= -1)\n",
    "            trace = K.sum(K.dot(K.dot(K.transpose(encoded), adj_normalize_tf), encoded) * K.eye(encoding_dim_1))/encoding_dim_1\n",
    "            return recon + e*trace\n",
    "\n",
    "        adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "        autoencoder.compile(optimizer = adam, loss = myloss)\n",
    "        autoencoder.fit(feature, feature, epochs=epoch,batch_size=feature.shape[0],shuffle=False,verbose=0)\n",
    "\n",
    "    #     encoded_node_1 = encoder.predict(feature,batch_size=feature.shape[0])\n",
    "\n",
    "    #     proj = TSNE(random_state=123).fit_transform(encoded_node_1)\n",
    "\n",
    "    #     y = onehot2index(load_data(dataset)[8])\n",
    "    #     scatter(proj, y)\n",
    "    #     plt.show()\n",
    "\n",
    "        trainX , trainY = encoder.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "        testX, testY = encoder.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "        knn = KNeighborsClassifier()\n",
    "        knn.fit(trainX,trainY)\n",
    "        pred_KNN = knn.predict(testX)\n",
    "        auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "        print(auc_KNN)\n",
    "        maxi = max(maxi, auc_KNN)\n",
    "        feature = encoder.predict(feature,batch_size=feature.shape[0])\n",
    "        encoding_dim_1 = max(int(encoding_dim_1/2), 512)\n",
    "    #     epoch = max(int(epoch/2), 100)\n",
    "    #     epoch = 100\n",
    "        if encoding_dim_1 > 512:\n",
    "            e = e/2/2\n",
    "            \n",
    "    acc.append(maxi)\n",
    "print('mean:%f'%np.mean(acc))\n",
    "print('std:%f'%np.std(acc))\n",
    "#         print('KNN_auc:%f'%auc_KNN)\n",
    "\n",
    "#         time.append(n)\n",
    "#         acc.append(auc_KNN)\n",
    "#         plt.plot(time, acc, 'b')\n",
    "#         plt.show()\n",
    "        ####邻接矩阵\n",
    "    #     adj = np.zeros([feature.shape[0],feature.shape[0]])\n",
    "    #     for i in range(feature.shape[0]):\n",
    "    #         adj[i] = np.exp(-np.sum(np.square(proj - proj[i]) ,axis=1))\n",
    "\n",
    "    #     adj = adj - np.eye(adj.shape[1])\n",
    "\n",
    "    #     adj_normalize = normalize_adj(adj) \n",
    "    #     adj_normalize_tf = tf.convert_to_tensor(adj_normalize.toarray(), dtype='float32')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(-feature[1]).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat传递方案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense  ,Dot ,Lambda, Dropout, BatchNormalization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model  \n",
    "from keras.datasets import mnist  \n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "import keras\n",
    "import numpy as np  \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt  \n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "K.clear_session()\n",
    "# 指定第一块GPU可用 \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)\n",
    "\n",
    "dataset = 'pubmed'  #citeseer   cora  pubmed\n",
    "acc = []\n",
    "for n in range(5):\n",
    "    feature = load_data(dataset)[1].toarray()\n",
    "    # original_train, original_test = feature[load_data(dataset)[5]], feature[load_data(dataset)[7]]\n",
    "    # x_train, x_test = feature[load_data(dataset)[5]], feature[load_data(dataset)[7]]\n",
    "    y_train = load_data(dataset)[2][load_data(dataset)[5]].tolist()\n",
    "    ####自带邻接矩阵\n",
    "    adj = load_data(dataset)[0]\n",
    "    adj_normalize = adj\n",
    "    adj_normalize_tf = tf.convert_to_tensor(adj_normalize.toarray(),dtype='float32')\n",
    "    #####逼近式\n",
    "    # alpha = 0.99\n",
    "    # adj = load_data(dataset)[0]\n",
    "    # adj_normalize = normalize_adj(adj)\n",
    "    # adj_normalize = np.linalg.inv(np.eye(adj.shape[0]) - 0.9*adj_normalize.toarray())\n",
    "    # adj_normalize = np.eye(adj.shape[0]) + alpha*adj_normalize + alpha**2*np.dot(adj_normalize,adj_normalize.T)\n",
    "\n",
    "    # adj_normalize_tf = tf.convert_to_tensor(adj_normalize,dtype='float32')\n",
    "    encoding_dim_1 = 512\n",
    "    encoding_dim_2 = 256\n",
    "\n",
    "    order_0 = Input(shape=(feature.shape[1],)) \n",
    "\n",
    "    def graph_convex(x):\n",
    "        x1 = K.concatenate([adj_normalize_tf,x], axis = -1)\n",
    "        return x1\n",
    "\n",
    "    ###########不对称\n",
    "    order_1 = Lambda(function=graph_convex)(order_0)\n",
    "    encoded_1 = Dense(encoding_dim_1, activation='relu', use_bias=True)(order_1)  \n",
    "    # encoded_1 = Dropout(0.5)(encoded_1)\n",
    "    # encoded_1 = BatchNormalization(axis = -1)(encoded_1)\n",
    "\n",
    "    order_2 = Lambda(function= graph_convex)(encoded_1)\n",
    "    encoded_2 = Dense(encoding_dim_2, activation='relu', use_bias=True)(order_2)  \n",
    "    # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "    # encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "\n",
    "    order_3 = Lambda(function= graph_convex)(encoded_2)\n",
    "    decoded_1 = Dense(encoding_dim_1, activation='relu', use_bias=True)(order_3)  \n",
    "    # decoded_1 = Dropout(0.5)(decoded_1)\n",
    "    # decoded_1 = BatchNormalization(axis = -1)(decoded_1)\n",
    "\n",
    "    order_4 = Lambda(function= graph_convex)(decoded_1)\n",
    "    decoded_2 = Dense(feature.shape[1], activation='relu', use_bias=True)(order_4)  \n",
    "    # decoded_2 = BatchNormalization(axis = -1)(decoded_2)\n",
    "    autoencoder = Model(inputs=order_0, outputs=decoded_2)  \n",
    "    encoder_1 = Model(inputs=order_0, outputs=encoded_1)    \n",
    "    encoder_2 = Model(inputs=order_0, outputs=encoded_2) \n",
    "    decoder_1 = Model(inputs=order_0, outputs=decoded_1)\n",
    "\n",
    "\n",
    "    ########重构loss#################\n",
    "    adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    autoencoder.compile(optimizer=adam, loss='binary_crossentropy')\n",
    "    autoencoder.fit(feature, feature, epochs=100,batch_size=feature.shape[0],shuffle=False)\n",
    "\n",
    "    encoded_node_1 = encoder_2.predict(feature,batch_size=feature.shape[0])\n",
    "\n",
    "    proj = TSNE(random_state=123).fit_transform(encoded_node_1)\n",
    "\n",
    "    y = onehot2index(load_data(dataset)[8])\n",
    "    scatter(proj, y)\n",
    "    plt.show()\n",
    "\n",
    "    trainX , trainY = encoder_2.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "    testX, testY = encoder_2.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(trainX,trainY)\n",
    "    pred_KNN = knn.predict(testX)\n",
    "    auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "    print('KNN_auc:%f'%auc_KNN)\n",
    "    acc.append(auc_KNN)\n",
    "    \n",
    "print('mean:%f'%np.mean(acc))\n",
    "print('std:%f'%np.std(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense  ,Dot ,Lambda, Dropout, BatchNormalization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model  \n",
    "from keras.datasets import mnist  \n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "import keras\n",
    "import numpy as np  \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt  \n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "K.clear_session()\n",
    "\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "# 指定第一块GPU可用 \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)\n",
    "\n",
    "dataset = 'cora'  #citeseer   cora  pubmed(每层加BN,20,4096)\n",
    "\n",
    "feature = load_data(dataset)[1].toarray()\n",
    "# original_train, original_test = feature[load_data(dataset)[5]], feature[load_data(dataset)[7]]\n",
    "# x_train, x_test = feature[load_data(dataset)[5]], feature[load_data(dataset)[7]]\n",
    "y_train = load_data(dataset)[2][load_data(dataset)[5]].tolist()\n",
    "####自带邻接矩阵\n",
    "adj = load_data(dataset)[0]\n",
    "adj_normalize = adj\n",
    "adj_normalize_tf = tf.convert_to_tensor(adj_normalize.toarray(),dtype='float32')\n",
    "#####逼近式\n",
    "# alpha = 0.99\n",
    "# adj = load_data(dataset)[0]\n",
    "# adj_normalize = normalize_adj(adj)\n",
    "# adj_normalize = np.linalg.inv(np.eye(adj.shape[0]) - 0.9*adj_normalize.toarray())\n",
    "# adj_normalize = np.eye(adj.shape[0]) + alpha*adj_normalize + alpha**2*np.dot(adj_normalize,adj_normalize.T)\n",
    "\n",
    "# adj_normalize_tf = tf.convert_to_tensor(adj_normalize,dtype='float32')\n",
    "encoding_dim_1 = 2048\n",
    "encoding_dim_2 = 512\n",
    "feature = np.concatenate([feature, adj.toarray()], axis = -1)\n",
    "acc = []\n",
    "for n in range(5):\n",
    "    order_0 = Input(shape=(feature.shape[1],)) \n",
    "\n",
    "\n",
    "    ###########不对称\n",
    "    encoded_1 = Dense(encoding_dim_1, activation='sigmoid', use_bias=True)(order_0)  \n",
    "    # encoded_1 = Dropout(0.5)(encoded_1)\n",
    "    encoded_1 = BatchNormalization(axis = -1)(encoded_1)\n",
    "\n",
    "    encoded_2 = Dense(encoding_dim_2, activation='sigmoid', use_bias=True)(encoded_1)  \n",
    "    # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "    encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "\n",
    "    decoded_1 = Dense(encoding_dim_1, activation='sigmoid', use_bias=True)(encoded_2)  \n",
    "    # decoded_1 = Dropout(0.5)(decoded_1)\n",
    "    decoded_1 = BatchNormalization(axis = -1)(decoded_1)\n",
    "\n",
    "    decoded_2 = Dense(feature.shape[1], activation='sigmoid', use_bias=True)(decoded_1)  \n",
    "    # decoded_2 = BatchNormalization(axis = -1)(decoded_2)\n",
    "    autoencoder = Model(inputs=order_0, outputs=decoded_2)  \n",
    "    encoder_1 = Model(inputs=order_0, outputs=encoded_1)    \n",
    "    encoder_2 = Model(inputs=order_0, outputs=encoded_2) \n",
    "    decoder_1 = Model(inputs=order_0, outputs=decoded_1)\n",
    "\n",
    "\n",
    "    ########重构loss#################\n",
    "    adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    autoencoder.compile(optimizer=adam, loss='binary_crossentropy')\n",
    "    autoencoder.fit(feature, feature, epochs=20,batch_size=feature.shape[0],shuffle=True,verbose=0)\n",
    "\n",
    "    # encoded_node_1 = encoder_2.predict(feature,batch_size=feature.shape[0])\n",
    "\n",
    "    # proj = TSNE(random_state=123).fit_transform(encoded_node_1)\n",
    "\n",
    "    # y = onehot2index(load_data(dataset)[8])\n",
    "    # scatter(proj, y)\n",
    "    # plt.show()\n",
    "\n",
    "    trainX , trainY = encoder_2.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "    testX, testY = encoder_2.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(trainX,trainY)\n",
    "    pred_KNN = knn.predict(testX)\n",
    "    auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "    print('KNN_auc:%f'%auc_KNN)\n",
    "    \n",
    "    acc.append(auc_KNN)\n",
    "    \n",
    "print('mean:%f'%np.mean(acc))\n",
    "print('std:%f'%np.std(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重构相似度矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense  ,Dot ,Lambda, Dropout, BatchNormalization\n",
    "from keras.models import Model  \n",
    "from keras.datasets import mnist  \n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "import keras\n",
    "import numpy as np  \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt  \n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import matplotlib.patheffects as PathEffects\n",
    "K.clear_session()\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "\n",
    "def scatter(x, colors):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", load_data(dataset)[4].shape[1]))\n",
    "#     print(palette)\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "# 指定第一块GPU可用 \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)\n",
    "\n",
    "dataset = 'cora'  #citeseer   cora  pubmed\n",
    "\n",
    "feature = load_data(dataset)[1].toarray()\n",
    "ACC=[0,0,0,0,0]\n",
    "for i in range(1):\n",
    "#####逼近式\n",
    "    alpha = 0.85  ###局部强度\n",
    "    adj = load_data(dataset)[0].toarray()\n",
    "    adj = adj.astype('float32')\n",
    "    adj_normalize = normalize_adj(adj)\n",
    "    adj_normalize = np.linalg.inv(np.eye(adj.shape[0]) - alpha*adj_normalize.toarray())\n",
    "    # adj_normalize = np.eye(adj.shape[0]) + alpha*adj_normalize + alpha**2*np.dot(adj_normalize,adj_normalize.T)\n",
    "    adj_normalize_tf = tf.convert_to_tensor(adj_normalize,dtype='float32')\n",
    "\n",
    "    ######penalty of degree\n",
    "    adj_tf = tf.convert_to_tensor(adj, dtype='float32')\n",
    "    # C = adj_normalize_tf\n",
    "    C = adj_tf\n",
    "    # C = adj_tf + K.dot(adj_tf, adj_tf) - K.dot(adj_tf, adj_tf)*K.eye(feature.shape[0])\n",
    "    D = K.dot(adj_tf, adj_tf)*K.eye(feature.shape[0])  ##degree matrix\n",
    "    D_pow = K.pow(D + 1e-10, -1)*K.eye(feature.shape[0])\n",
    "\n",
    "    D_norm = K.pow(D + 1e-20, -0.5)*K.eye(feature.shape[0]) ####D^-0.5\n",
    "    W = K.dot(K.dot(D_pow, C), D_pow)\n",
    "    W = K.dot(K.dot(D_norm, W), D_norm)\n",
    "    L = K.eye(feature.shape[0]) - W\n",
    "\n",
    "    encoding_dim_1 = 2048\n",
    "    encoding_dim_2 = 512\n",
    "    t = 1.0/4  ##高斯核系数\n",
    "    rate = 0.1  ###相似度矩阵学习率\n",
    "    iter_n = []\n",
    "    acc = []\n",
    "    # order_0 = Input(shape=(feature.shape[1],)) \n",
    "\n",
    "    # def graph_convex(x):\n",
    "    #     x1 = K.dot(adj_normalize_tf,x)\n",
    "    #     return x1\n",
    "\n",
    "    # ###########不对称\n",
    "    # order_1 = Lambda(function=graph_convex)(order_0)\n",
    "    # encoded_1 = Dense(encoding_dim_1, activation='tanh', use_bias=True)(order_1)  \n",
    "    # # encoded_1 = Dropout(0.5)(encoded_1)\n",
    "    # encoded_1 = BatchNormalization(axis = -1)(encoded_1)\n",
    "\n",
    "    # order_2 = Lambda(function= graph_convex)(encoded_1)\n",
    "    # encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(order_2)  \n",
    "    # # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "    # encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "\n",
    "    # order_3 = Lambda(function= graph_convex)(encoded_2)\n",
    "    # decoded_1 = Dense(encoding_dim_1, activation='tanh', use_bias=True)(order_3)  \n",
    "    # # decoded_1 = Dropout(0.5)(decoded_1)\n",
    "    # decoded_1 = BatchNormalization(axis = -1)(decoded_1)\n",
    "\n",
    "    # order_4 = Lambda(function= graph_convex)(decoded_1)\n",
    "    # decoded_2 = Dense(feature.shape[1], activation='tanh', use_bias=True)(order_4)  \n",
    "    # # decoded_2 = BatchNormalization(axis = -1)(decoded_2)\n",
    "\n",
    "    # autoencoder = Model(inputs=order_0, outputs=decoded_2)  \n",
    "    # encoder_1 = Model(inputs=order_0, outputs=encoded_1)    \n",
    "    # encoder_2 = Model(inputs=order_0, outputs=encoded_2) \n",
    "    # decoder_1 = Model(inputs=order_0, outputs=decoded_1)\n",
    "\n",
    "    # adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for n in tqdm(range(20)):\n",
    "        D_vect = np.sqrt(np.sum(adj,axis=0))\n",
    "        order_0 = Input(shape=(feature.shape[1],)) \n",
    "\n",
    "        def graph_convex(x):\n",
    "            x1 = K.dot(adj_normalize_tf,x)\n",
    "            return x1\n",
    "\n",
    "    #     order_1 = Lambda(function=graph_convex)(order_0)\n",
    "    #     encoded_1 = Dense(encoding_dim_1, activation='tanh', use_bias=True)(order_1)  \n",
    "    # #     encoded_1 = Dropout(0.5)(encoded_1)\n",
    "    #     encoded_1 = BatchNormalization(axis = -1)(encoded_1)\n",
    "    ###七层############\n",
    "    #     order_2 = Lambda(function= graph_convex)(order_0)\n",
    "    #     order_2 = BatchNormalization(axis = -1)(order_2)\n",
    "    #     encoded_22 = Dense(2048, activation='tanh', use_bias=True)(order_2)  \n",
    "    # #     encoded_2 = Dropout(0.5)(encoded_2)\n",
    "    # #     encoded_22 = BatchNormalization(axis = -1)(encoded_22)\n",
    "\n",
    "    #     order_2 = Lambda(function= graph_convex)(encoded_22)\n",
    "    #     order_2 = BatchNormalization(axis = -1)(order_2)\n",
    "    #     encoded_22 = Dense(1024, activation='tanh', use_bias=True)(order_2)  \n",
    "    # #     encoded_2 = Dropout(0.5)(encoded_2)\n",
    "    # #     encoded_22 = BatchNormalization(axis = -1)(encoded_22)\n",
    "\n",
    "    #     order_2 = Lambda(function= graph_convex)(encoded_22)\n",
    "    #     order_2 = BatchNormalization(axis = -1)(order_2)\n",
    "    #     encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(order_2)  \n",
    "    # #     encoded_2 = Dropout(0.5)(encoded_2)\n",
    "    # #     encoded_2 = BatchNormalization(axis = -1)(encoded_22)\n",
    "\n",
    "    # #     order_3 = Lambda(function= graph_convex)(encoded_22)\n",
    "    # #     decoded_1 = Dense(encoding_dim_1, activation='tanh', use_bias=True)(order_3)  \n",
    "    # # #     decoded_1 = Dropout(0.5)(decoded_1)\n",
    "    # #     decoded_1 = BatchNormalization(axis = -1)(decoded_1)\n",
    "\n",
    "    #     order_4 = Lambda(function= graph_convex)(encoded_2)\n",
    "    #     order_4 = BatchNormalization(axis = -1)(order_4)\n",
    "    #     decoded_2 = Dense(1024, activation='tanh', use_bias=True)(order_4)  \n",
    "    # #     decoded_2 = BatchNormalization(axis = -1)(decoded_2)\n",
    "\n",
    "    #     order_4 = Lambda(function= graph_convex)(decoded_2)\n",
    "    #     order_4 = BatchNormalization(axis = -1)(order_4)\n",
    "    #     decoded_2 = Dense(2048, activation='tanh', use_bias=True)(order_4)  \n",
    "    # #     decoded_2 = BatchNormalization(axis = -1)(decoded_2)\n",
    "\n",
    "    #     order_4 = Lambda(function= graph_convex)(decoded_2)\n",
    "    #     order_4 = BatchNormalization(axis = -1)(order_4)\n",
    "    #     decoded_2 = Dense(feature.shape[1], activation='sigmoid', use_bias=True)(order_4)  \n",
    "    #     decoded_2 = BatchNormalization(axis = -1)(decoded_2)\n",
    "        ###########不对称\n",
    "        order_1 = Lambda(function=graph_convex)(order_0)\n",
    "    #     order_1 = BatchNormalization(axis = -1)(order_1)\n",
    "        encoded_1 = Dense(encoding_dim_1, activation='tanh', use_bias=True)(order_1)  \n",
    "    #     encoded_1 = Dropout(0.5)(encoded_1)\n",
    "        encoded_1 = BatchNormalization(axis = -1)(encoded_1)\n",
    "\n",
    "        order_2 = Lambda(function= graph_convex)(encoded_1)\n",
    "    #     order_2 = BatchNormalization(axis = -1)(order_2)\n",
    "        encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(order_2)  \n",
    "    #     encoded_2 = Dropout(0.5)(encoded_2)\n",
    "        encoded_22 = BatchNormalization(axis = -1)(encoded_2)\n",
    "\n",
    "        order_3 = Lambda(function= graph_convex)(encoded_22)\n",
    "    #     order_3 = BatchNormalization(axis = -1)(order_3)\n",
    "        decoded_1 = Dense(encoding_dim_1, activation='tanh', use_bias=True)(order_3)  \n",
    "    #     decoded_1 = Dropout(0.5)(decoded_1)\n",
    "        decoded_1 = BatchNormalization(axis = -1)(decoded_1)\n",
    "\n",
    "        order_4 = Lambda(function= graph_convex)(decoded_1)\n",
    "    #     order_4 = BatchNormalization(axis = -1)(order_4)\n",
    "        decoded_2 = Dense(feature.shape[1], activation='sigmoid', use_bias=True)(order_4)  \n",
    "    #     decoded_2 = BatchNormalization(axis = -1)(decoded_2)\n",
    "\n",
    "        def myloss(y_true, y_pred, e= 1e-5):\n",
    "            recon = K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "            trace = K.sum(K.dot(K.dot(K.transpose(encoded_2), L), encoded_2) * K.eye(encoding_dim_2))/encoding_dim_2\n",
    "    #         orth = K.mean(K.square(K.dot(K.dot(K.transpose(encoded_2), D), encoded_2) - K.eye(encoding_dim_2)))\n",
    "            return recon + e*trace\n",
    "\n",
    "        autoencoder = Model(inputs=order_0, outputs=decoded_2)  \n",
    "    #     encoder_1 = Model(inputs=order_0, outputs=encoded_1)    \n",
    "        encoder_2 = Model(inputs=order_0, outputs=encoded_2) \n",
    "    #     decoder_1 = Model(inputs=order_0, outputs=decoded_1)\n",
    "\n",
    "        adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "        autoencoder.compile(optimizer=adam, loss=myloss)\n",
    "        autoencoder.fit(feature, feature, epochs=300,batch_size=feature.shape[0],shuffle=False,verbose=1)\n",
    "\n",
    "        encoded_node_1 = encoder_2.predict(feature,batch_size=feature.shape[0])\n",
    "\n",
    "        proj3 = TSNE(random_state=123,perplexity=30,early_exaggeration=12,init='random').fit_transform(encoded_node_1)\n",
    "\n",
    "        y = onehot2index(load_data(dataset)[8])\n",
    "    #     plt.subplot(133)\n",
    "        scatter(proj3, y)\n",
    "    #     plt.savefig('/home/lifuzhen/gcn/visulization.png')\n",
    "        plt.show()\n",
    "\n",
    "        trainX , trainY = encoder_2.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "        testX, testY = encoder_2.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "        knn = KNeighborsClassifier()\n",
    "        knn.fit(trainX,trainY)\n",
    "        pred_KNN = knn.predict(testX)\n",
    "        auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "        print('KNN_auc:%f'%auc_KNN)\n",
    "        iter_n.append(n)\n",
    "        acc.append(auc_KNN)\n",
    "        plt.plot(iter_n, acc, 'b')\n",
    "        plt.show()\n",
    "\n",
    "        ###高斯核邻接矩阵t-sne\n",
    "\n",
    "    #     adj_learn = np.zeros([feature.shape[0],feature.shape[0]])\n",
    "    #     for i in range(feature.shape[0]):\n",
    "    #         adj_learn[i] = np.exp(-np.sum(np.square(proj - proj[i])/2.0/t/t ,axis=1))\n",
    "\n",
    "#         ###高斯核邻接矩阵embedding\n",
    "        adj_learn = np.zeros([feature.shape[0],feature.shape[0]])\n",
    "        for i in range(feature.shape[0]):\n",
    "            adj_learn[i] = np.exp(-np.mean(     np.transpose(np.transpose(np.square(encoded_node_1 - encoded_node_1[i]))*D_vect)*D_vect[i]/t ,axis=1       ))\n",
    "                ###局部密度高斯核邻接矩阵embedding\n",
    "#         adj_learn = np.zeros([feature.shape[0],feature.shape[0]])\n",
    "#         for i in range(feature.shape[0]):\n",
    "#             for j in range(feature.shape[0]):\n",
    "#                 adj_learn[i][j] = np.exp(-np.mean(np.square(encoded_node_1[i] - encoded_node_1[j])*D_vect[i]*D_vect[j]/t))\n",
    "\n",
    "    #     ####sigmoid核\n",
    "    #     adj_learn = np.zeros([feature.shape[0],feature.shape[0]])\n",
    "    #     for i in range(feature.shape[0]):\n",
    "    #         adj_learn[i] = 2.0/(1 + np.exp(np.sum(np.square(encoded_node_1 - encoded_node_1[i])/2.0/t/t ,axis=1)))\n",
    "\n",
    "        adj_learn = adj_learn - np.eye(adj_learn.shape[1])\n",
    "        adj_delta = adj_learn - adj\n",
    "    # #     cmp = np.abs(adj_delta)\n",
    "    # # #     index_keep = np.where(cmp >= 0.5)\n",
    "    # #     index_alter = np.where(cmp < 0.7)\n",
    "    # #     adj[index_alter] = adj_learn[index_alter]\n",
    "    # #     index_keep = np.where(adj_delta >= 0.5 or adj_delta <= -0.5)\n",
    "        margin = 0.4\n",
    "    #     delta = 0.01\n",
    "        index_alter_P = np.where((0 < adj_delta)&(adj_delta < margin))\n",
    "        index_alter_N = np.where((-margin < adj_delta)& (adj_delta< 0))\n",
    "        adj[index_alter_P] += adj_delta[index_alter_P]/2.0\n",
    "        adj[index_alter_N] += adj_delta[index_alter_N]/2.0\n",
    "#         adj[index_alter_P] += 0.01\n",
    "#         adj[index_alter_N] -= 0.01\n",
    "\n",
    "    #     model_metric = keras.Sequential()\n",
    "    #     model_metric.add(Dense(512, activation='sigmoid', use_bias=True, input_dim = 512))\n",
    "    #     model_metric.add(Dense(1024, activation='sigmoid', use_bias=True))\n",
    "    #     model_metric.add(Dense(feature.shape[0], activation='sigmoid', use_bias=True))\n",
    "    #     model_metric.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    #     model_metric.fit(encoded_node_1, adj, epochs=500, batch_size=256, shuffle=True, verbose=1)\n",
    "\n",
    "    #     adj = model_metric.predict(encoded_node_1,batch_size=encoded_node_1.shape[0])\n",
    "    #     adj = (1-rate)*adj + rate*(adj_learn-np.eye(adj_learn.shape[1]))\n",
    "        adj_normalize = normalize_adj(adj) ######rate or (1-rate)&rate\n",
    "        adj_normalize = np.linalg.inv(np.eye(adj.shape[0]) - alpha*adj_normalize.toarray())\n",
    "        adj_normalize_tf = tf.convert_to_tensor(adj_normalize, dtype='float32')\n",
    "    #     adj_tf = tf.convert_to_tensor(adj, dtype='float32')\n",
    "    #     C = adj_tf + K.dot(adj_tf, adj_tf) - K.dot(adj_tf, adj_tf)*K.eye(feature.shape[0])\n",
    "    #     D = K.dot(adj_tf, adj_tf)*K.eye(feature.shape[0])  ##degree matrix\n",
    "    #     D_pow = K.pow(D + 1e-10, -1)*K.eye(feature.shape[0])\n",
    "\n",
    "    #     D_norm = K.pow(D + 1e-20, -0.5)*K.eye(feature.shape[0]) ####D^-0.5\n",
    "    #     W = K.dot(K.dot(D_pow, C), D_pow)\n",
    "    #     W = K.dot(K.dot(D_norm, W), D_norm)\n",
    "    #     L = K.eye(feature.shape[0]) - W\n",
    "\n",
    "    print('最高准确率：%f'%max(acc))\n",
    "    print('平均准确率：%f'%(sum(acc)/len(acc)))\n",
    "    ACC += acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense  ,Dot ,Lambda, Dropout, BatchNormalization\n",
    "from keras.models import Model  \n",
    "from keras.datasets import mnist  \n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "import keras\n",
    "import numpy as np  \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt  \n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import seaborn as sn\n",
    "K.clear_session()\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "\n",
    "def scatter(x, colors):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", load_data(dataset)[4].shape[1]))\n",
    "#     print(palette)\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "# 指定第一块GPU可用 \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)\n",
    "\n",
    "dataset = 'pubmed'  #citeseer   cora  pubmed\n",
    "\n",
    "feature = load_data(dataset)[1].toarray()\n",
    "ACC=[0,0,0,0,0]\n",
    "for i in range(1):\n",
    "#####逼近式\n",
    "    alpha = 0.95  ###局部强度\n",
    "#     t = 1.0/8 \n",
    "    adj = load_data(dataset)[0].toarray()\n",
    "#     distance = np.zeros([feature.shape[0],feature.shape[0]])\n",
    "#     for i in range(feature.shape[0]):\n",
    "#         distance[i] = np.sqrt(np.sum(np.square(feature - feature[i]) ,axis=1)) ###计算欧氏距离\n",
    "            \n",
    "#     Top_10 = np.sort(distance,axis=1)[:,1:51]\n",
    "        \n",
    "#     sigma = np.std(Top_10,axis=1)\n",
    "#     ######高斯核\n",
    "#     adj = np.zeros([feature.shape[0],feature.shape[0]])\n",
    "#     for i in range(feature.shape[0]):\n",
    "#         adj[i] = np.exp(-np.mean( np.transpose(np.transpose(np.square(feature - feature[i]))/sigma)/sigma[i]/t ,axis=1       ))\n",
    "    \n",
    "    ######线性核\n",
    "#     adj = np.zeros([feature.shape[0],feature.shape[0]])\n",
    "#     for i in range(feature.shape[0]):\n",
    "#         adj[i] = np.dot(feature/np.sqrt(np.sum(np.square(feature),axis=1,keepdims=True)),feature[i]/np.sqrt(np.sum(np.square(feature[i]))))\n",
    "#     adj = adj - np.eye(adj.shape[1])\n",
    "    \n",
    "    adj = adj.astype('float32')\n",
    "    adj_normalize = normalize_adj(adj)\n",
    "    adj_normalize = np.linalg.inv(np.eye(adj.shape[0]) - alpha*adj_normalize.toarray())\n",
    "    # adj_normalize = np.eye(adj.shape[0]) + alpha*adj_normalize + alpha**2*np.dot(adj_normalize,adj_normalize.T)\n",
    "    adj_normalize_tf = tf.convert_to_tensor(adj_normalize,dtype='float32')\n",
    "\n",
    "    ######penalty of degree\n",
    "#     adj_tf = tf.convert_to_tensor(adj, dtype='float32')\n",
    "#     # C = adj_normalize_tf\n",
    "#     C = adj_tf\n",
    "#     # C = adj_tf + K.dot(adj_tf, adj_tf) - K.dot(adj_tf, adj_tf)*K.eye(feature.shape[0])\n",
    "#     D = K.dot(adj_tf, adj_tf)*K.eye(feature.shape[0])  ##degree matrix\n",
    "#     D_pow = K.pow(D + 1e-10, -1)*K.eye(feature.shape[0])\n",
    "\n",
    "#     D_norm = K.pow(D + 1e-20, -0.5)*K.eye(feature.shape[0]) ####D^-0.5\n",
    "#     W = K.dot(K.dot(D_pow, C), D_pow)\n",
    "#     W = K.dot(K.dot(D_norm, W), D_norm)\n",
    "#     L = K.eye(feature.shape[0]) - W\n",
    "#     L_array = K.eval(L)\n",
    "\n",
    "    encoding_dim_1 = 1024\n",
    "    encoding_dim_2 = 256\n",
    "    t = 1.0/8  ##高斯核系数\n",
    "    rate = 0.1  ###相似度矩阵学习率\n",
    "    iter_n = []\n",
    "    acc = []\n",
    "    if dataset == 'citeseer':\n",
    "        onehot = load_data(dataset)[8]\n",
    "        isolated = np.where(np.sum(onehot,axis=-1)==0)\n",
    "        for i in isolated[0]:\n",
    "            onehot[i] = np.copy(onehot[adj.tolist()[i].index(1)])\n",
    "    else:\n",
    "        onehot = load_data(dataset)[8]\n",
    "    y = onehot2index(onehot)\n",
    "    Mat_block = np.zeros_like(adj)\n",
    "    y = onehot2index(onehot)\n",
    "    for i in range(onehot.shape[1]):\n",
    "        for j in np.where(y==i)[0]:\n",
    "            Mat_block[np.where(y==i)[0],j] = 1\n",
    "    Mat_block = Mat_block - np.eye(adj.shape[0])\n",
    "    Mat_block_array = Mat_block\n",
    "#     Mat_block = tf.convert_to_tensor(Mat_block, dtype='float32')\n",
    "\n",
    "\n",
    "#     for n in tqdm(range(20)):\n",
    "#         D_vect = np.sqrt(np.sum(adj,axis=0))\n",
    "    order_0 = Input(shape=(feature.shape[1],)) \n",
    "\n",
    "    def graph_convex(x):\n",
    "        x1 = K.dot(adj_normalize_tf,x)\n",
    "        return x1\n",
    "\n",
    "    ###########不对称\n",
    "    order_1 = Lambda(function=graph_convex)(order_0)\n",
    "#     order_1 = BatchNormalization(axis = -1)(order_1)\n",
    "    encoded_1 = Dense(encoding_dim_1, activation='tanh', use_bias=True)(order_1)  \n",
    "#     encoded_1 = Dropout(0.5)(encoded_1)\n",
    "    encoded_1 = BatchNormalization(axis = -1)(encoded_1)\n",
    "\n",
    "    order_2 = Lambda(function= graph_convex)(encoded_1)\n",
    "#     order_2 = BatchNormalization(axis = -1)(order_2)\n",
    "    encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(order_2)\n",
    "#     Res = Dense(encoding_dim_2, activation='relu', use_bias=True)(order_0)   \n",
    "#     encoded_2 = keras.layers.Add()([encoded_2,Res])\n",
    "#     encoded_2 = Dropout(0.5)(encoded_2)\n",
    "    encoded_22 = BatchNormalization(axis = -1)(encoded_2)\n",
    "\n",
    "    order_3 = Lambda(function= graph_convex)(encoded_22)\n",
    "#     order_3 = BatchNormalization(axis = -1)(order_3)\n",
    "    decoded_1 = Dense(encoding_dim_1, activation='tanh', use_bias=True)(order_3)  \n",
    "#     decoded_1 = Dropout(0.5)(decoded_1)\n",
    "    decoded_1 = BatchNormalization(axis = -1)(decoded_1)\n",
    "\n",
    "    order_4 = Lambda(function= graph_convex)(decoded_1)\n",
    "#     order_4 = BatchNormalization(axis = -1)(order_4)\n",
    "    decoded_2 = Dense(feature.shape[1], activation='sigmoid', use_bias=True)(order_4)  \n",
    "#     decoded_2 = BatchNormalization(axis = -1)(decoded_2)\n",
    "\n",
    "    def myloss(y_true, y_pred, e= 1e-5):\n",
    "        recon = K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "#         trace = K.sum(K.dot(K.dot(K.transpose(encoded_2), L), encoded_2) * K.eye(encoding_dim_2))/encoding_dim_2\n",
    "#         block = K.sqrt(K.mean(K.square(Mat_block - K.dot(encoded_2,K.transpose(encoded_2)))))\n",
    "#         block = K.sum(K.binary_crossentropy(Mat_block, K.dot(K.l2_normalize(encoded_2,axis=-1),K.transpose(K.l2_normalize(encoded_2,axis=-1)))))\n",
    "#         orth = K.mean(K.square(K.dot(K.dot(K.transpose(encoded_2), D), encoded_2) - K.eye(encoding_dim_2)))\n",
    "        return recon \n",
    "\n",
    "    autoencoder = Model(inputs=order_0, outputs=decoded_2)  \n",
    "#     encoder_1 = Model(inputs=order_0, outputs=encoded_1)    \n",
    "    encoder_2 = Model(inputs=order_0, outputs=encoded_2) \n",
    "#     decoder_1 = Model(inputs=order_0, outputs=decoded_1)\n",
    "    test = Model(inputs=order_0, outputs=order_1)\n",
    "    EP = 50\n",
    "#     D_vect = np.sqrt(np.sum(adj,axis=0))\n",
    "    for n in tqdm(range(21)):\n",
    "\n",
    "        #块效应邻接矩阵\n",
    "       \n",
    "        adj_reindex = np.eye(adj.shape[0])\n",
    "        count = 0\n",
    "        for i in range(onehot.shape[1]):\n",
    "            adj_reindex[count:count+np.where(y==i)[0].shape[0]] = np.copy(adj[np.where(y==i)])\n",
    "            count = count + np.where(y==i)[0].shape[0]\n",
    "        alt = np.copy(np.transpose(adj_reindex))\n",
    "        count = 0\n",
    "        for i in range(onehot.shape[1]):\n",
    "            adj_reindex[count:count+np.where(y==i)[0].shape[0]] = np.copy(alt[np.where(y==i)])\n",
    "            count = count + np.where(y==i)[0].shape[0]\n",
    "\n",
    "        fig = plt.figure() #调用figure创建一个绘图对象\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(adj_reindex, vmin=0, vmax=1, cmap='Reds_r')  #绘制热力图，从-1到1\n",
    "        fig.colorbar(cax)  #将matshow生成热力图设置为颜色渐变条\n",
    "        ticks = np.arange(0,9,1) #生成0-9，步长为1\n",
    "        if n%5 == 0:\n",
    "            plt.savefig('/home/lifuzhen/gcn/%s_unrefined_%d.eps'%(dataset,n))\n",
    "        plt.show()\n",
    "        #块效应邻接矩阵 end\n",
    "#         D_vect = np.power(np.sum(adj,axis=0),1/10)\n",
    "        \n",
    "        adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "        autoencoder.compile(optimizer=adam, loss=myloss)\n",
    "        autoencoder.fit(feature, feature, epochs=EP,batch_size=feature.shape[0],shuffle=False,verbose=1)\n",
    "\n",
    "        encoded_node_1 = encoder_2.predict(feature,batch_size=feature.shape[0])\n",
    "\n",
    "        proj3 = TSNE(random_state=123,perplexity=30,early_exaggeration=12,init='random').fit_transform(encoded_node_1)\n",
    "\n",
    "        y = onehot2index(onehot)\n",
    "    #     plt.subplot(133)\n",
    "        fig = plt.figure()\n",
    "        scatter(proj3, y)\n",
    "    #     plt.savefig('/home/lifuzhen/gcn/visulization.png')\n",
    "        plt.show()\n",
    "\n",
    "        trainX , trainY = encoder_2.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "        testX, testY = encoder_2.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "        knn = KNeighborsClassifier()\n",
    "        knn.fit(trainX,trainY)\n",
    "        pred_KNN = knn.predict(testX)\n",
    "        auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "\n",
    "        confusion = metrics.confusion_matrix(testY , pred_KNN)\n",
    "        confusion_matrix = confusion/np.sum(confusion,axis=-1,keepdims=1)\n",
    "        np.round(confusion_matrix,3)\n",
    "        sn.heatmap(confusion_matrix,annot=True,cmap='Blues',linecolor='black',linewidths=0.1)\n",
    "        if n%5 == 0:\n",
    "            plt.savefig('/home/lifuzhen/gcn/%s_confusion_matrix_%d.eps'%(dataset,n))\n",
    "        plt.show()\n",
    "        print('KNN_auc:%f'%auc_KNN)\n",
    "        iter_n.append(n)\n",
    "        acc.append(auc_KNN)\n",
    "        plt.plot(iter_n, acc, 'b')\n",
    "        plt.xticks(range(1,len(acc)+1))\n",
    "        plt.show()\n",
    "        autoencoder.save_weights('GDN.h5')\n",
    "        K.clear_session()\n",
    "        ######计算方差#########\n",
    "#         distance = np.zeros([feature.shape[0],feature.shape[0]])\n",
    "#         for i in range(feature.shape[0]):\n",
    "#             distance[i] = np.sqrt(np.sum(np.square(encoded_node_1 - encoded_node_1[i]) ,axis=1)) ###计算欧氏距离\n",
    "            \n",
    "#         Top_10 = np.sort(distance,axis=1)[:,1:11]\n",
    "        \n",
    "#         sigma = np.std(Top_10,axis=1)\n",
    "#         #####计算方差end####################\n",
    "\n",
    "# #         ###高斯核邻接矩阵embedding 度方案@#########\n",
    "# #         adj_learn = np.zeros([feature.shape[0],feature.shape[0]])\n",
    "# #         for i in range(feature.shape[0]):\n",
    "# #             adj_learn[i] = np.exp(-np.mean(     np.transpose(np.transpose(np.square(encoded_node_1 - encoded_node_1[i]))*D_vect)*D_vect[i]/t ,axis=1       ))\n",
    "       \n",
    "#         #######高斯核邻接矩阵embedding 方差估计方案#######\n",
    "#         adj_learn = np.zeros([feature.shape[0],feature.shape[0]])\n",
    "#         for i in range(feature.shape[0]):\n",
    "#             adj_learn[i] = np.exp(-np.mean( np.transpose(np.transpose(np.square(encoded_node_1 - encoded_node_1[i]))/sigma)/sigma[i]/t ,axis=1))\n",
    "        adj_learn = np.zeros([feature.shape[0],feature.shape[0]])\n",
    "        for i in range(feature.shape[0]):\n",
    "            adj_learn[i] = np.copy(np.dot(encoded_node_1/np.sqrt(np.sum(np.square(encoded_node_1),axis=1,keepdims=True)),encoded_node_1[i]/np.sqrt(np.sum(np.square(encoded_node_1[i])))))\n",
    "        \n",
    "        adj_learn[adj_learn<0 ] = 0\n",
    "        adj_learn = adj_learn - np.eye(adj_learn.shape[1])\n",
    "        adj_delta = adj_learn - adj\n",
    "\n",
    "#         margin = 0.5\n",
    "#     #     delta = 0.01\n",
    "#         index_alter_P = np.where((0 < adj_delta)&(adj_delta < margin))\n",
    "#         index_alter_N = np.where((-margin < adj_delta)& (adj_delta< 0))\n",
    "#         adj[index_alter_P] += adj_delta[index_alter_P]/2.0\n",
    "#         adj[index_alter_N] += adj_delta[index_alter_N]/2.0\n",
    "#         adj[index_alter_P] += 0.05\n",
    "#         adj[index_alter_N] -= 0.05\n",
    "\n",
    "#         adj = model_metric.predict(encoded_node_1,batch_size=encoded_node_1.shape[0])\n",
    "        adj = (1-rate)*adj + rate*(adj_learn-np.eye(adj_learn.shape[1]))\n",
    "        adj[adj<0.09] = 0\n",
    "        adj_normalize_0 = normalize_adj(adj)\n",
    "        adj_normalize = normalize_adj(adj) ######rate or (1-rate)&rate\n",
    "        adj_normalize = np.linalg.inv(np.eye(adj.shape[0]) - alpha*adj_normalize.toarray())\n",
    "        adj_normalize_tf = tf.convert_to_tensor(adj_normalize, dtype='float32')\n",
    "        \n",
    "        \n",
    "        order_0 = Input(shape=(feature.shape[1],)) \n",
    "\n",
    "        def graph_convex(x):\n",
    "            x1 = K.dot(adj_normalize_tf,x)\n",
    "            return x1\n",
    "\n",
    "        ###########不对称\n",
    "        order_1 = Lambda(function=graph_convex)(order_0)\n",
    "#         order_1 = BatchNormalization(axis = -1)(order_1)\n",
    "        encoded_1 = Dense(encoding_dim_1, activation='tanh', use_bias=True)(order_1)  \n",
    "    #     encoded_1 = Dropout(0.5)(encoded_1)\n",
    "        encoded_1 = BatchNormalization(axis = -1)(encoded_1)\n",
    "\n",
    "        order_2 = Lambda(function= graph_convex)(encoded_1)\n",
    "#         order_2 = BatchNormalization(axis = -1)(order_2)\n",
    "        encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(order_2)\n",
    "#         Res = Dense(encoding_dim_2, activation='relu', use_bias=True)(order_0)   \n",
    "#         encoded_2 = keras.layers.Add()([encoded_2,Res])\n",
    "    #     encoded_2 = Dropout(0.5)(encoded_2)\n",
    "        encoded_22 = BatchNormalization(axis = -1)(encoded_2)\n",
    "\n",
    "        order_3 = Lambda(function= graph_convex)(encoded_22)\n",
    "#         order_3 = BatchNormalization(axis = -1)(order_3)\n",
    "        decoded_1 = Dense(encoding_dim_1, activation='tanh', use_bias=True)(order_3)  \n",
    "    #     decoded_1 = Dropout(0.5)(decoded_1)\n",
    "        decoded_1 = BatchNormalization(axis = -1)(decoded_1)\n",
    "\n",
    "        order_4 = Lambda(function= graph_convex)(decoded_1)\n",
    "#         order_4 = BatchNormalization(axis = -1)(order_4)\n",
    "        decoded_2 = Dense(feature.shape[1], activation='sigmoid', use_bias=True)(order_4)  \n",
    "    #     decoded_2 = BatchNormalization(axis = -1)(decoded_2)\n",
    "#         Mat_block = tf.convert_to_tensor(Mat_block_array, dtype='float32')\n",
    "        def myloss(y_true, y_pred, e= 1e-5):\n",
    "            recon = K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "#             trace = K.sum(K.dot(K.dot(K.transpose(encoded_2), K.variable(L_array)), encoded_2) * K.eye(encoding_dim_2))/encoding_dim_2\n",
    "#             block = K.sqrt(K.mean(K.square(Mat_block - K.dot(encoded_2,K.transpose(encoded_2)))))\n",
    "#             block = K.sum(K.binary_crossentropy(Mat_block, K.dot(K.l2_normalize(encoded_2,axis=-1),K.transpose(K.l2_normalize(encoded_2,axis=-1)))))\n",
    "#         orth = K.mean(K.square(K.dot(K.dot(K.transpose(encoded_2), D), encoded_2) - K.eyle(encoding_dim_2)))\n",
    "            return recon \n",
    "\n",
    "        autoencoder = Model(inputs=order_0, outputs=decoded_2)  \n",
    "    #     encoder_1 = Model(inputs=order_0, outputs=encoded_1)    \n",
    "        encoder_2 = Model(inputs=order_0, outputs=encoded_2) \n",
    "    #     decoder_1 = Model(inputs=order_0, outputs=decoded_1)\n",
    "        test = Model(inputs=order_0, outputs=order_1)\n",
    "        autoencoder.load_weights('GDN.h5')\n",
    "        EP = 3\n",
    "#         print(test.predict(feature, batch_size=feature.shape[0]))\n",
    "    print('最高准确率：%f'%max(acc))\n",
    "    print('平均准确率：%f'%(sum(acc)/len(acc)))\n",
    "    ACC += acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = metrics.confusion_matrix(testY , pred_KNN)\n",
    "confusion_matrix = confusion/np.sum(confusion,axis=-1,keepdims=1)\n",
    "np.round(confusion_matrix,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(load_data(dataset)[0].toarray()[3212]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in isolated[0]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(adj_normalize_0.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'citeseer'\n",
    "if dataset == 'citeseer':\n",
    "    onehot = load_data(dataset)[8]\n",
    "    isolated = np.where(np.sum(onehot,axis=-1)==0)\n",
    "    for i in isolated[0]:\n",
    "        onehot[i] = np.copy(onehot[load_data(dataset)[0].toarray().tolist()[i].index(1)])\n",
    "else:\n",
    "    onehot = load_data(dataset)[8]\n",
    "y = onehot2index(onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = load_data('citeseer')[1].toarray()\n",
    "\n",
    "# distance = np.zeros([feature.shape[0],feature.shape[0]])\n",
    "# for i in range(feature.shape[0]):\n",
    "#     distance[i] = np.sqrt(np.sum(np.square(feature - feature[i]) ,axis=1)) ###计算欧氏距离\n",
    "\n",
    "# Top_10 = np.sort(distance,axis=1)[:,1:51]\n",
    "\n",
    "# sigma = np.std(Top_10,axis=1)\n",
    "# ######高斯核\n",
    "# adj = np.zeros([feature.shape[0],feature.shape[0]])\n",
    "# for i in range(feature.shape[0]):\n",
    "#     adj[i] = np.exp(-np.mean( np.transpose(np.transpose(np.square(feature - feature[i]))/sigma)/sigma[i]/t ,axis=1       ))\n",
    "\n",
    "#####线性核\n",
    "adj = np.zeros([feature.shape[0],feature.shape[0]])\n",
    "for i in range(feature.shape[0]):\n",
    "    adj[i] = np.dot(feature/(np.sqrt(np.sum(np.square(feature),axis=1,keepdims=True))+1e-10),feature[i]/(1e-10+np.sqrt(np.sum(np.square(feature[i])))))\n",
    "adj = adj - np.eye(adj.shape[1])\n",
    "adj[adj<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_reindex = np.eye(adj.shape[0])\n",
    "QQQQ = np.eye(adj.shape[0])\n",
    "n = 0\n",
    "for i in range(6):\n",
    "    adj_reindex[n:n+np.where(y==i)[0].shape[0]] = adj[np.where(y==i)]\n",
    "    n = n + np.where(y==i)[0].shape[0]\n",
    "alt = np.copy(np.transpose(adj_reindex))\n",
    "n = 0\n",
    "for i in range(6):\n",
    "    adj_reindex[n:n+np.where(y==i)[0].shape[0]] = alt[np.where(y==i)]\n",
    "    n = n + np.where(y==i)[0].shape[0]\n",
    "\n",
    "fig = plt.figure() #调用figure创建一个绘图对象\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(adj_reindex, vmin=0, vmax=1, cmap='Reds_r')  #绘制热力图，从-1到1\n",
    "fig.colorbar(cax)  #将matshow生成热力图设置为颜色渐变条\n",
    "ticks = np.arange(0,9,1) #生成0-9，步长为1\n",
    "plt.savefig('/home/lifuzhen/gcn/citeseer_unrefined_original.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.sum(onehot,axis=-1)==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id(adj_reindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id(QQQQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "# correlations = data.corr()  #计算变量之间的相关系数矩阵\n",
    "# plot correlation matrix\n",
    "fig = plt.figure() #调用figure创建一个绘图对象\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion_matrix, vmin=0, vmax=1, cmap='Reds_r')  #绘制热力图，从-1到1\n",
    "fig.colorbar(cax)  #将matshow生成热力图设置为颜色渐变条\n",
    "ticks = numpy.arange(0,9,1) #生成0-9，步长为1\n",
    "plt.savefig('/home/lifuzhen/gcn/refined.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "sn.heatmap(confusion_matrix,annot=True,cmap='Blues',linecolor='black',linewidths=0.1)\n",
    "plt.savefig('/home/lifuzhen/gcn/confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "# correlations = data.corr()  #计算变量之间的相关系数矩阵\n",
    "# plot correlation matrix\n",
    "fig = plt.figure() #调用figure创建一个绘图对象\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion, vmin=0, vmax=1, cmap='Reds_r')  #绘制热力图，从-1到1\n",
    "fig.colorbar(cax)  #将matshow生成热力图设置为颜色渐变条\n",
    "ticks = numpy.arange(0,9,1) #生成0-9，步长为1\n",
    "plt.savefig('/home/lifuzhen/gcn/refined.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.predict(feature, batch_size=feature.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = [0.744,0.747,0.756,0.753,0.763]\n",
    "plt.plot(range(1,6),acc)\n",
    "plt.xlim(1,5)\n",
    "plt.ylim(0.72,0.78)\n",
    "plt.xlabel('iteration times')\n",
    "plt.ylabel('accuracy')\n",
    "plt.savefig('/home/lifuzhen/gcn/refeff.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "adj = load_data('pubmed')[0].toarray()\n",
    "G=nx.Graph()\n",
    "for i in range(adj.shape[0]):\n",
    "    for j in range(adj.shape[1]):\n",
    "    #     G.add_nodes_from(adj)\n",
    "        if adj[i][j] == 1:\n",
    "            G.add_edge(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = load_data('pubmed')[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "labels = np.argmax(labels,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G,node_color=labels,with_labels=False,node_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import scipy.io as sio\n",
    "adj = load_data('pubmed')[0]\n",
    "sio.savemat(\"/home/lifuzhen/SDNE/GraphData/pubmed.mat\", {\"traingraph_sparse\":adj})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = sio.loadmat('/home/liyouru/.ipython/profile_myserver/code/AAAI_2019/ADJ451.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = adj['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sio.loadmat(\"/home/lifuzhen/SDNE/GraphData/451.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sio.loadmat(\"/home/lifuzhen/SDNE/result/blogCatalog-Mon-Oct-15-12:29:01-2018/embedding.mat\")['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_adjlist(G,path='/home/lifuzhen/deepwalk/karate.adjlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = []\n",
    "for line in open(\"/home/lifuzhen/deepwalk/karate.embedding\"): \n",
    "    embedding.extend(line.strip().split(' '))\n",
    "embedding = map(float, embedding)\n",
    "embedding = list(embedding)\n",
    "embedding.pop(0)\n",
    "embedding.pop(0)\n",
    "\n",
    "import numpy as np\n",
    "embedding = np.array(embedding)\n",
    "embedding = embedding.reshape([-1,257])\n",
    "representations = np.zeros([embedding.shape[0],embedding.shape[1]-1])\n",
    "for i in range(embedding.shape[0]):\n",
    "    representations[int(embedding[i][0])] = embedding[i][1:257]\n",
    "# print(representations[1358])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense  ,Dot ,Lambda, Dropout, BatchNormalization\n",
    "from keras.models import Model  \n",
    "from keras.datasets import mnist  \n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "import keras\n",
    "import numpy as np  \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt  \n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import matplotlib.patheffects as PathEffects\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "\n",
    "def scatter(x, colors):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", load_data(dataset)[4].shape[1]))\n",
    "#     print(palette)\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "acc = []\n",
    "for n in range(1):\n",
    "    dataset = 'pubmed'  #citeseer   cora  pubmed\n",
    "\n",
    "    trainX , trainY = representations[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "    testX, testY = representations[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "    svc = SVC(C=3,kernel='rbf')#cora 1.7 citeseer 3\n",
    "    gbdt = GradientBoostingClassifier(n_estimators=8)\n",
    "    rf = RandomForestClassifier()\n",
    "    knn = KNeighborsClassifier()\n",
    "\n",
    "    svc.fit(trainX, trainY)\n",
    "    gbdt.fit(trainX, trainY)\n",
    "    rf.fit(trainX, trainY)\n",
    "    knn.fit(trainX,trainY)\n",
    "\n",
    "    pred_SVC = svc.predict(testX)\n",
    "    pred_GBDT = gbdt.predict(testX)\n",
    "    pred_RF = rf.predict(testX)\n",
    "    pred_KNN = knn.predict(testX)\n",
    "\n",
    "    auc_SVC = metrics.accuracy_score(testY , pred_SVC)\n",
    "    auc_GBDT = metrics.accuracy_score(testY , pred_GBDT)\n",
    "    auc_RF = metrics.accuracy_score(testY , pred_RF)\n",
    "    auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "    acc.append(auc_KNN)\n",
    "    \n",
    "    print('SVC_auc：%f'%auc_SVC)\n",
    "    print('GBDT_auc：%f'%auc_GBDT)\n",
    "    print('RF_auc：%f'%auc_RF)\n",
    "    print('KNN_auc:%f'%auc_KNN)\n",
    "#     proj1 = TSNE(random_state=123).fit_transform(representations)\n",
    "\n",
    "#     y = onehot2index(load_data(dataset)[8])\n",
    "#     scatter(proj1, y)\n",
    "#     plt.show()\n",
    "print('mean:%f'%np.mean(acc))\n",
    "print('std:%f'%np.std(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "# correlations = data.corr()  #计算变量之间的相关系数矩阵\n",
    "# plot correlation matrix\n",
    "fig = plt.figure() #调用figure创建一个绘图对象\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(adj[0:20,0:20], vmin=0, vmax=1, cmap='Reds_r')  #绘制热力图，从-1到1\n",
    "fig.colorbar(cax)  #将matshow生成热力图设置为颜色渐变条\n",
    "ticks = numpy.arange(0,9,1) #生成0-9，步长为1\n",
    "plt.savefig('/home/lifuzhen/gcn/unrefined.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = TSNE(random_state=1,perplexity=10,early_exaggeration=12,init='random').fit_transform(encoded_node_1)\n",
    "    \n",
    "y = onehot2index(load_data(dataset)[8])\n",
    "scatter(proj, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(-np.sort(-adj)).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_node_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(-np.sort(-adj_learn)).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_delta[index_alter_N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('平均准确率：%f'%(sum(acc)/len(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(adj.toarray() == 6.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 降噪+L1正则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense  ,Dot ,Lambda, Dropout, BatchNormalization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model  \n",
    "from keras.datasets import mnist  \n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "import keras\n",
    "import numpy as np  \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt  \n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "# 指定第一块GPU可用 \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)\n",
    "\n",
    "dataset = 'cora'  #citeseer   cora  pubmed\n",
    "\n",
    "feature = load_data(dataset)[1].toarray()\n",
    "feature_noise = feature + 0.3*np.random.normal(loc=0.0, scale= 1.0, size= feature.shape)\n",
    "feature_noise = np.clip(feature_noise, 0, 1)\n",
    "# original_train, original_test = feature[load_data(dataset)[5]], feature[load_data(dataset)[7]]\n",
    "# x_train, x_test = feature[load_data(dataset)[5]], feature[load_data(dataset)[7]]\n",
    "y_train = load_data(dataset)[2][load_data(dataset)[5]].tolist()\n",
    "####自带邻接矩阵\n",
    "# adj = load_data(dataset)[0]\n",
    "# adj_normalize = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "# adj_normalize_tf = tf.convert_to_tensor(adj_normalize.toarray(),dtype='float32')\n",
    "#####逼近式\n",
    "alpha = 0.9\n",
    "adj = load_data(dataset)[0]\n",
    "adj_normalize = normalize_adj(adj)\n",
    "adj_normalize = np.linalg.inv(np.eye(adj.shape[0]) - 0.9*adj_normalize.toarray())\n",
    "# adj_normalize = np.eye(adj.shape[0]) + alpha*adj_normalize + alpha**2*np.dot(adj_normalize,adj_normalize.T)\n",
    "adj_normalize_tf = tf.convert_to_tensor(adj_normalize,dtype='float32')\n",
    "\n",
    "encoding_dim_1 = 512\n",
    "encoding_dim_2 = 512\n",
    "encoding_dim_3 = 512\n",
    "\n",
    "\n",
    "order_0 = Input(shape=(feature.shape[1],)) \n",
    "\n",
    "def graph_convex(x):\n",
    "    x1 = K.dot(adj_normalize_tf,x)\n",
    "    return x1\n",
    "\n",
    "###########不对称\n",
    "order_1 = Lambda(function=graph_convex)(order_0)\n",
    "encoded_1 = Dense(encoding_dim_1, activation='tanh', use_bias=True)(order_1)  \n",
    "# encoded_1 = Dropout(0.5)(encoded_1)\n",
    "# encoded_1 = BatchNormalization(axis = -1)(encoded_1)\n",
    "\n",
    "order_2 = Lambda(function= graph_convex)(encoded_1)\n",
    "encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(order_2)  \n",
    "# encoded_2 = Dropout(0.5)(encoded_2)\n",
    "# encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "\n",
    "order_3 = Lambda(function= graph_convex)(encoded_2)\n",
    "encoded_3 = Dense(encoding_dim_3, activation='tanh', use_bias=True)(order_3)  \n",
    "# encoded_2 = Dropout(0.5)(encoded_2)\n",
    "encoded_3 = BatchNormalization(axis = -1)(encoded_3)\n",
    "\n",
    "order_4 = Lambda(function= graph_convex)(encoded_3)\n",
    "decoded_1 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(order_4)  \n",
    "# decoded_1 = Dropout(0.5)(decoded_1)\n",
    "# decoded_1 = BatchNormalization(axis = -1)(decoded_1)\n",
    "\n",
    "order_5 = Lambda(function= graph_convex)(decoded_1)\n",
    "decoded_2 = Dense(encoding_dim_1, activation='tanh', use_bias=True)(order_5)  \n",
    "# decoded_1 = Dropout(0.5)(decoded_1)\n",
    "# decoded_2 = BatchNormalization(axis = -1)(decoded_2)\n",
    "\n",
    "order_6 = Lambda(function= graph_convex)(decoded_2)\n",
    "decoded_3 = Dense(feature.shape[1], activation='tanh', use_bias=True)(order_6)  \n",
    "# decoded_2 = Dense(feature.shape[1], activation='sigmoid', use_bias=True, activity_regularizer=regularizers.l1(10e-7))(order_4)  \n",
    "# decoded_2 = BatchNormalization(axis = -1)(decoded_2)\n",
    "\n",
    "autoencoder = Model(inputs=order_0, outputs=decoded_3)  \n",
    "encoder_1 = Model(inputs=order_0, outputs=encoded_1)    \n",
    "encoder_2 = Model(inputs=order_0, outputs=encoded_2) \n",
    "encoder_3 = Model(inputs=order_0, outputs=encoded_3)\n",
    "decoder_1 = Model(inputs=order_0, outputs=decoded_1)\n",
    "decoder_2 = Model(inputs=order_0, outputs=decoded_2)\n",
    "\n",
    "\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "autoencoder.compile(optimizer=adam, loss='binary_crossentropy')\n",
    "autoencoder.fit(feature, feature, epochs=5000,batch_size=feature.shape[0],shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图传递"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense  ,Dot ,Lambda, Dropout, BatchNormalization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model  \n",
    "from keras.datasets import mnist  \n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "import keras\n",
    "import numpy as np  \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt  \n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "K.clear_session()\n",
    "# 指定第一块GPU可用 \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)\n",
    "\n",
    "selec = [0.75]\n",
    "ACC = []\n",
    "for alpha in selec:\n",
    "    acc = []\n",
    "    for n in tqdm(range(1)):\n",
    "        dataset = 'citeseer'  #citeseer   cora  pubmed\n",
    "\n",
    "        feature = load_data(dataset)[1].toarray()\n",
    "        # original_train, original_test = feature[load_data(dataset)[5]], feature[load_data(dataset)[7]]\n",
    "        # x_train, x_test = feature[load_data(dataset)[5]], feature[load_data(dataset)[7]]\n",
    "        y_train = load_data(dataset)[2][load_data(dataset)[5]].tolist()\n",
    "        ####自带邻接矩阵\n",
    "    #     adj = load_data(dataset)[0]\n",
    "    #     adj_normalize = normalize_adj(adj) + sp.eye(adj.shape[0])\n",
    "    #     adj_normalize_tf = tf.convert_to_tensor(adj_normalize.toarray(),dtype='float32')\n",
    "        #####逼近式\n",
    "    #     alpha = 0.85\n",
    "        adj = load_data(dataset)[0]\n",
    "#         adj = np.zeros([feature.shape[0],feature.shape[0]])\n",
    "#         for i in range(feature.shape[0]):\n",
    "#             adj[i] = np.dot(feature/np.sqrt(np.sum(np.square(feature),axis=1,keepdims=True)),feature[i]/np.sqrt(np.sum(np.square(feature[i]))))\n",
    "#         adj = np.zeros([feature.shape[0],feature.shape[0]])\n",
    "#         for i in range(feature.shape[0]):\n",
    "#             adj[i] = np.exp(-np.mean( np.square(encoded_node_1 - encoded_node_1[i]) ,axis=1))\n",
    "       \n",
    "        adj_normalize = normalize_adj(adj)\n",
    "        adj_normalize = np.linalg.inv(np.eye(adj.shape[0]) - alpha*adj_normalize.toarray())\n",
    "        # adj_normalize = np.eye(adj.shape[0]) + alpha*adj_normalize + alpha**2*np.dot(adj_normalize,adj_normalize.T)\n",
    "        adj_normalize_tf = tf.convert_to_tensor(adj_normalize,dtype='float32')\n",
    "    #     #####逼近式\n",
    "    #     alpha = 0.75\n",
    "    #     adj = load_data(dataset)[0]\n",
    "    #     adj_normalize = normalize_adj(adj)\n",
    "    #     adj_normalize = np.eye(adj.shape[0])*np.dot(adj_normalize.toarray(),adj_normalize.toarray()) + \\\n",
    "    #     np.dot(alpha*adj_normalize.toarray(),np.linalg.inv(np.eye(adj.shape[0]) - alpha*adj_normalize.toarray()))\n",
    "    #     # adj_normalize = np.eye(adj.shape[0]) + alpha*adj_normalize + alpha**2*np.dot(adj_normalize,adj_normalize.T)\n",
    "    #     adj_normalize_tf = tf.convert_to_tensor(adj_normalize,dtype='float32')\n",
    "        #####逼近式\n",
    "    #     alpha = 0.9\n",
    "    #     adj = load_data(dataset)[0]\n",
    "    #     adj_normalize = normalize_adj(adj)\n",
    "    #     adj_normalize = np.eye(adj.shape[0]) + np.dot(alpha*adj_normalize.toarray(), np.linalg.inv(np.eye(adj.shape[0]) - alpha*alpha*np.dot(adj_normalize.toarray(),adj_normalize.toarray()))) ##citeseer 0.75 cora0.9 pubmed 0.95\n",
    "    #     # adj_normalize = np.eye(adj.shape[0]) + alpha*adj_normalize + alpha**2*np.dot(adj_normalize,adj_normalize.T)\n",
    "    #     adj_normalize_tf = tf.convert_to_tensor(adj_normalize,dtype='float32')\n",
    "\n",
    "        ######penalty of degree\n",
    "#         adj_tf = tf.convert_to_tensor(adj.toarray(), dtype='float32')\n",
    "#         C = adj_tf + K.dot(adj_tf, adj_tf) - K.dot(adj_tf, adj_tf)*K.eye(feature.shape[0])\n",
    "#         D = K.dot(adj_tf, adj_tf)*K.eye(feature.shape[0])  ##degree matrix\n",
    "#         D_pow = K.pow(D + 1e-10, -2)*K.eye(feature.shape[0])\n",
    "\n",
    "#         D_norm = K.pow(D + 1e-20, -0.5)*K.eye(feature.shape[0]) ####D^-0.5\n",
    "#         W = K.dot(K.dot(D_pow, C), D_pow)\n",
    "#         W = K.dot(K.dot(D_norm, W), D_norm)\n",
    "#         L = K.eye(feature.shape[0]) - W\n",
    "#         L = K.eval(L)\n",
    "#         L = normalize_adj(L).toarray()\n",
    "#         L[np.where(np.isnan(L))] = 0.\n",
    "#         L = tf.convert_to_tensor(L, dtype='float32')\n",
    "        ###高斯核邻接矩阵\n",
    "        # t = 3\n",
    "        # adj = np.zeros([feature.shape[0],feature.shape[0]])\n",
    "        # for i in range(feature.shape[0]):\n",
    "        #     adj[i] = np.exp(-np.sum(np.square(feature - feature[i])/2/t ,axis=1))\n",
    "\n",
    "        # adj_normalize = normalize_adj(adj)\n",
    "        # adj_normalize_tf = tf.convert_to_tensor(adj_normalize.toarray(),dtype='float32')\n",
    "        ####citeseer########\n",
    "        # encoding_dim_1 = 2048\n",
    "        # encoding_dim_2 = 512\n",
    "        ####cora #######\n",
    "        encoding_dim_1 = 1024\n",
    "        encoding_dim_2 = 512\n",
    "        ######pubmed#####\n",
    "        # encoding_dim_1 = 128\n",
    "        # encoding_dim_2 = 128\n",
    "\n",
    "        order_0 = Input(shape=(feature.shape[1],)) \n",
    "\n",
    "        def graph_convex(x):\n",
    "            x1 = K.dot(adj_normalize_tf,x)\n",
    "            return x1\n",
    "\n",
    "        ###########不对称\n",
    "        order_1 = Lambda(function=graph_convex)(order_0)\n",
    "#         encoded_1 = Dense(encoding_dim_1, activation='tanh', use_bias=True)(order_1)  \n",
    "#         # encoded_1 = Dropout(0.5)(encoded_1)\n",
    "#         encoded_1 = BatchNormalization(axis = -1)(encoded_1)\n",
    "\n",
    "        order_2 = Lambda(function= graph_convex)(order_1)\n",
    "        encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(order_2)  \n",
    "        # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "        encoded_22 = BatchNormalization(axis = -1)(encoded_2)\n",
    "        \n",
    "#         encoded_2 = Lambda(function= graph_convex)(encoded_2)\n",
    "#         encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_2)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "        \n",
    "#         encoded_2 = Lambda(function= graph_convex)(encoded_2)\n",
    "#         encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_2)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "        \n",
    "#         encoded_2 = Lambda(function= graph_convex)(encoded_2)\n",
    "#         encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_2)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "        \n",
    "#         encoded_2 = Lambda(function= graph_convex)(encoded_2)\n",
    "#         encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_2)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "        \n",
    "#         encoded_2 = Lambda(function= graph_convex)(encoded_2)\n",
    "#         encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_2)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "        \n",
    "#         encoded_2 = Lambda(function= graph_convex)(encoded_2)\n",
    "#         encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_2)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "        \n",
    "#         encoded_2 = Lambda(function= graph_convex)(encoded_2)\n",
    "#         encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_2)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "        \n",
    "#         encoded_2 = Lambda(function= graph_convex)(encoded_2)\n",
    "#         encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_2)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "        \n",
    "#         encoded_2 = Lambda(function= graph_convex)(encoded_2)\n",
    "#         encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_2)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "        \n",
    "#         encoded_2 = Lambda(function= graph_convex)(encoded_2)\n",
    "#         encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_2)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "        \n",
    "#         encoded_2 = Lambda(function= graph_convex)(encoded_2)\n",
    "#         encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_2)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "        \n",
    "#         encoded_2 = Lambda(function= graph_convex)(encoded_2)\n",
    "#         encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_2)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "        \n",
    "#         encoded_2 = Lambda(function= graph_convex)(encoded_2)\n",
    "#         encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_2)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "        \n",
    "#         encoded_2 = Lambda(function= graph_convex)(encoded_2)\n",
    "#         encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_2)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "        \n",
    "#         encoded_2 = Lambda(function= graph_convex)(encoded_2)\n",
    "#         encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_2)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "        \n",
    "#         encoded_2 = Lambda(function= graph_convex)(encoded_2)\n",
    "#         encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_2)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "        \n",
    "#         encoded_2 = Lambda(function= graph_convex)(encoded_2)\n",
    "#         encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_2)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "        \n",
    "#         encoded_2 = Lambda(function= graph_convex)(encoded_2)\n",
    "#         encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_2)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "        \n",
    "#         encoded_2 = Lambda(function= graph_convex)(encoded_2)\n",
    "#         encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_2)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "        \n",
    "#         encoded_2 = Lambda(function= graph_convex)(encoded_2)\n",
    "#         encoded_2 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_2)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_22 = BatchNormalization(axis = -1)(encoded_2)\n",
    "        \n",
    "#         encoded_22 = Lambda(function= graph_convex)(encoded_22)\n",
    "#         encoded_22 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_22)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_22 = BatchNormalization(axis = -1)(encoded_22)\n",
    "        \n",
    "#         encoded_22 = Lambda(function= graph_convex)(encoded_22)\n",
    "#         encoded_22 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_22)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_22 = BatchNormalization(axis = -1)(encoded_22)\n",
    "        \n",
    "#         encoded_22 = Lambda(function= graph_convex)(encoded_22)\n",
    "#         encoded_22 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_22)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_22 = BatchNormalization(axis = -1)(encoded_22)\n",
    "        \n",
    "#         encoded_22 = Lambda(function= graph_convex)(encoded_22)\n",
    "#         encoded_22 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_22)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_22 = BatchNormalization(axis = -1)(encoded_22)\n",
    "        \n",
    "#         encoded_22 = Lambda(function= graph_convex)(encoded_22)\n",
    "#         encoded_22 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_22)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_22 = BatchNormalization(axis = -1)(encoded_22)\n",
    "        \n",
    "#         encoded_22 = Lambda(function= graph_convex)(encoded_22)\n",
    "#         encoded_22 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_22)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_22 = BatchNormalization(axis = -1)(encoded_22)\n",
    "        \n",
    "#         encoded_22 = Lambda(function= graph_convex)(encoded_22)\n",
    "#         encoded_22 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_22)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_22 = BatchNormalization(axis = -1)(encoded_22)\n",
    "        \n",
    "#         encoded_22 = Lambda(function= graph_convex)(encoded_22)\n",
    "#         encoded_22 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_22)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_22 = BatchNormalization(axis = -1)(encoded_22)\n",
    "        \n",
    "#         encoded_22 = Lambda(function= graph_convex)(encoded_22)\n",
    "#         encoded_22 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_22)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_22 = BatchNormalization(axis = -1)(encoded_22)\n",
    "        \n",
    "#         encoded_22 = Lambda(function= graph_convex)(encoded_22)\n",
    "#         encoded_22 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_22)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_22 = BatchNormalization(axis = -1)(encoded_22)\n",
    "        \n",
    "#         encoded_22 = Lambda(function= graph_convex)(encoded_22)\n",
    "#         encoded_22 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_22)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_22 = BatchNormalization(axis = -1)(encoded_22)\n",
    "        \n",
    "#         encoded_22 = Lambda(function= graph_convex)(encoded_22)\n",
    "#         encoded_22 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_22)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_22 = BatchNormalization(axis = -1)(encoded_22)\n",
    "        \n",
    "#         encoded_22 = Lambda(function= graph_convex)(encoded_22)\n",
    "#         encoded_22 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_22)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_22 = BatchNormalization(axis = -1)(encoded_22)\n",
    "        \n",
    "#         encoded_22 = Lambda(function= graph_convex)(encoded_22)\n",
    "#         encoded_22 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_22)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_22 = BatchNormalization(axis = -1)(encoded_22)\n",
    "        \n",
    "#         encoded_22 = Lambda(function= graph_convex)(encoded_22)\n",
    "#         encoded_22 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_22)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_22 = BatchNormalization(axis = -1)(encoded_22)\n",
    "        \n",
    "#         encoded_22 = Lambda(function= graph_convex)(encoded_22)\n",
    "#         encoded_22 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_22)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_22 = BatchNormalization(axis = -1)(encoded_22)\n",
    "        \n",
    "#         encoded_22 = Lambda(function= graph_convex)(encoded_22)\n",
    "#         encoded_22 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_22)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_22 = BatchNormalization(axis = -1)(encoded_22)\n",
    "        \n",
    "#         encoded_22 = Lambda(function= graph_convex)(encoded_22)\n",
    "#         encoded_22 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_22)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_22 = BatchNormalization(axis = -1)(encoded_22)\n",
    "        \n",
    "#         encoded_22 = Lambda(function= graph_convex)(encoded_22)\n",
    "#         encoded_22 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_22)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_22 = BatchNormalization(axis = -1)(encoded_22)\n",
    "        \n",
    "#         encoded_22 = Lambda(function= graph_convex)(encoded_22)\n",
    "#         encoded_22 = Dense(encoding_dim_2, activation='tanh', use_bias=True)(encoded_22)  \n",
    "#         # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "#         encoded_22 = BatchNormalization(axis = -1)(encoded_22)\n",
    "\n",
    "#         order_3 = Lambda(function= graph_convex)(encoded_22)\n",
    "#         decoded_1 = Dense(encoding_dim_1, activation='tanh', use_bias=True)(order_3)  \n",
    "#         # decoded_1 = Dropout(0.5)(decoded_1)\n",
    "#         decoded_1 = BatchNormalization(axis = -1)(decoded_1)\n",
    "\n",
    "        order_4 = Lambda(function= graph_convex)(encoded_22)\n",
    "        decoded_2 = Dense(feature.shape[1], activation='sigmoid', use_bias=True)(order_4)  \n",
    "    #     decoded_2 = BatchNormalization(axis = -1)(decoded_2)\n",
    "\n",
    "\n",
    "        def myloss(y_true, y_pred, e= 1e-4):\n",
    "                recon = K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "#                 trace = K.sum(K.dot(K.dot(K.transpose(encoded_2), L), encoded_2) * K.eye(encoding_dim_2))/encoding_dim_2\n",
    "                return recon\n",
    "\n",
    "\n",
    "        ############对称\n",
    "        # order_1 = Lambda(function=graph_convex)(order_0)\n",
    "        # encoded_1M = Dense(encoding_dim_1, activation='tanh', use_bias=False)\n",
    "        # encoded_1 = encoded_1M(order_1) \n",
    "        # # encoded_1 = Dropout(0.5)(encoded_1)\n",
    "        # encoded_1 = BatchNormalization(axis = -1)(encoded_1)\n",
    "\n",
    "        # order_2 = Lambda(function= graph_convex)(encoded_1)\n",
    "        # encoded_2M = Dense(encoding_dim_2, activation='tanh', use_bias=False)\n",
    "        # encoded_2 = encoded_2M(order_2)\n",
    "        # # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "        # encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "\n",
    "        # order_3 = Lambda(function= graph_convex)(encoded_2)\n",
    "        # decoded_1 = Lambda(lambda x: K.tanh(K.dot(x,K.variable(np.transpose(encoded_2M.get_weights()[0]),dtype='float32'))))(order_3)  \n",
    "        # # decoded_1 = Dropout(0.5)(decoded_1)\n",
    "        # decoded_1 = BatchNormalization(axis = -1)(decoded_1)\n",
    "\n",
    "        # order_4 = Lambda(function= graph_convex)(decoded_1)\n",
    "        # decoded_2 = Lambda(lambda x: K.tanh(K.dot(x,K.variable(np.transpose(encoded_1M.get_weights()[0]),dtype='float32'))))(order_4)  \n",
    "        # decoded_2 = BatchNormalization(axis = -1)(decoded_2)\n",
    "\n",
    "        ###############直接用关系矩阵###########\n",
    "        # order_0 = Input(shape=(adj_normalize.shape[1],)) \n",
    "        # def graph_convex(x):\n",
    "        #     x1 = x\n",
    "        #     return x1\n",
    "        # order_1 = Lambda(function=graph_convex)(order_0)\n",
    "        # encoded_1M = Dense(encoding_dim_1, activation='tanh', use_bias=False)\n",
    "        # encoded_1 = encoded_1M(order_1) \n",
    "        # # encoded_1 = Dropout(0.5)(encoded_1)\n",
    "        # encoded_1 = BatchNormalization(axis = -1)(encoded_1)\n",
    "\n",
    "        # order_2 = Lambda(function= graph_convex)(encoded_1)\n",
    "        # encoded_2M = Dense(feature.shape[1], activation='tanh', use_bias=False)\n",
    "        # encoded_2 = encoded_2M(order_2)\n",
    "        # # encoded_2 = Dropout(0.5)(encoded_2)\n",
    "        # encoded_2 = BatchNormalization(axis = -1)(encoded_2)\n",
    "\n",
    "        # order_3 = Lambda(function= graph_convex)(encoded_2)\n",
    "        # decoded_1 = Lambda(lambda x: K.tanh(K.dot(x,K.variable(np.transpose(encoded_2M.get_weights()[0]),dtype='float32'))))(order_3)  \n",
    "        # # decoded_1 = Dropout(0.5)(decoded_1)\n",
    "        # decoded_1 = BatchNormalization(axis = -1)(decoded_1)\n",
    "\n",
    "        # order_4 = Lambda(function= graph_convex)(decoded_1)\n",
    "        # decoded_2 = Lambda(lambda x: K.tanh(K.dot(x,K.variable(np.transpose(encoded_1M.get_weights()[0]),dtype='float32'))))(order_4)  \n",
    "        # decoded_2 = BatchNormalization(axis = -1)(decoded_2)\n",
    "        # def myloss(x):\n",
    "        #     t = 1\n",
    "        #     size = adj_normalize.shape[0]\n",
    "        #     CE1, CE2, CE3 = 0, 0, 0\n",
    "        #     for i in range(size):\n",
    "        #         for j in range(i,size):\n",
    "        #             CE1 += K.binary_crossentropy(K.exp(-K.sum(K.square(K.gather(x[0],i) - K.gather(x[0], j)))/2.0/t), adj_normalize_tf[i,j])\n",
    "        #             CE2 += K.binary_crossentropy(K.exp(-K.sum(K.square(K.gather(x[1],i) - K.gather(x[1], j)))/2.0/t), adj_normalize_tf[i,j])\n",
    "        #             CE3 += K.binary_crossentropy(K.exp(-K.sum(K.square(K.gather(x[2],i) - K.gather(x[2], j)))/2.0/t), adj_normalize_tf[i,j])\n",
    "        #     CE4 = K.sum(K.binary_crossentropy(x[3],x[4]))\n",
    "        #     return CE1 + CE2 + CE3 + CE4\n",
    "\n",
    "        # def myloss(x):\n",
    "        #     CE1 = K.mean(K.binary_crossentropy(K.dot(x[0] , K.transpose(x[0])), adj_normalize_tf))\n",
    "        #     CE2 = K.mean(K.binary_crossentropy(K.dot(x[1] , K.transpose(x[1])), adj_normalize_tf))\n",
    "        #     CE3 = K.mean(K.binary_crossentropy(K.dot(x[2] , K.transpose(x[2])), adj_normalize_tf))\n",
    "        #     CE4 = K.mean(K.binary_crossentropy(x[3],x[4]))\n",
    "        #     return CE1 + CE2 + CE3 + CE4\n",
    "        # def myloss(x):\n",
    "        #     t = 1\n",
    "        #     adj_1 = K.exp(-K.sum(K.square(x[0] - K.reshape(x[0], [K.int_shape(x[0])[0],1,K.int_shape(x[0])[1]])),axis=-1)/2.0/t)\n",
    "        #     adj_2 = K.exp(-K.sum(K.square(x[1] - K.reshape(x[1], [K.int_shape(x[1])[0],1,K.int_shape(x[1])[1]])),axis=-1)/2.0/t)\n",
    "        #     adj_3 = K.exp(-K.sum(K.square(x[2] - K.reshape(x[2], [K.int_shape(x[2])[0],1,K.int_shape(x[2])[1]])),axis=-1)/2.0/t)\n",
    "\n",
    "        #     return K.mean(K.binary_crossentropy(adj_1 , adj_normalize_tf)) + K.mean(K.binary_crossentropy(adj_2 , adj_normalize_tf)) + K.mean(K.binary_crossentropy(adj_3 , adj_normalize_tf)) + K.mean(K.binary_crossentropy(x[3] , K.variable(feature,dtype='float32')))\n",
    "\n",
    "        # t = 1\n",
    "        # Loss = Lambda(lambda x :K.mean(K.binary_crossentropy(K.exp(-K.sum(K.square(x[0] - K.reshape(x[0], [K.int_shape(x[0])[0],1,K.int_shape(x[0])[1]])),axis=-1)/2.0/t) , adj_normalize_tf))\\\n",
    "        #                          +K.mean(K.binary_crossentropy(K.exp(-K.sum(K.square(x[1] - K.reshape(x[1], [K.int_shape(x[1])[0],1,K.int_shape(x[1])[1]])),axis=-1)/2.0/t) , adj_normalize_tf))\\\n",
    "        #                         +K.mean(K.binary_crossentropy(K.exp(-K.sum(K.square(x[2] - K.reshape(x[2], [K.int_shape(x[2])[0],1,K.int_shape(x[2])[1]])),axis=-1)/2.0/t) , adj_normalize_tf))\\\n",
    "        #                         +K.mean(K.binary_crossentropy(x[3],x[4])) )([encoded_1,encoded_2,decoded_1,decoded_2,order_0])\n",
    "        # Loss = Lambda(function=myloss)([encoded_1,encoded_2,decoded_1,decoded_2,order_0])\n",
    "        # def myloss(y_true, y_pred):\n",
    "        #     y_true = K.clip(y_true, K.epsilon(), 1)\n",
    "        #     y_pred = K.clip(y_pred, K.epsilon(), 1)\n",
    "        #     return K.sum(y_true * K.log(y_true / y_pred), axis=-1) + \n",
    "        autoencoder = Model(inputs=order_0, outputs=decoded_2)  \n",
    "#         encoder_1 = Model(inputs=order_0, outputs=encoded_1)    \n",
    "        encoder_2 = Model(inputs=order_0, outputs=encoded_2) \n",
    "#         decoder_1 = Model(inputs=order_0, outputs=decoded_1)\n",
    "\n",
    "\n",
    "\n",
    "        #####结构保持loss + 重构loss############\n",
    "        # autoencoder.compile(optimizer='Adam', loss=lambda y_true,y_pred:y_pred)\n",
    "        ########重构loss#################\n",
    "        adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "        autoencoder.compile(optimizer=adam, loss=myloss)\n",
    "        autoencoder.fit(feature, feature, epochs=300,batch_size=feature.shape[0],shuffle=False, verbose=1)\n",
    "        ##cora: 50  citeseer:   pubmed: \n",
    "\n",
    "        ########三层########\n",
    "        # encoder_2.compile(optimizer=adam , loss='binary_crossentropy')\n",
    "        # encoder_2.fit(feature, feature, epochs=300,batch_size=feature.shape[0],shuffle=False)\n",
    "        from sklearn.svm import SVC\n",
    "        from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "        from sklearn.ensemble.forest import RandomForestClassifier\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        from sklearn import metrics\n",
    "        def onehot2index(onehot):\n",
    "            index = []\n",
    "            onehot = onehot.tolist()\n",
    "            for i in onehot:\n",
    "                index.append(i.index(1))\n",
    "            return np.array(index)\n",
    "\n",
    "        trainX , trainY = encoder_2.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "        testX, testY = encoder_2.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "        # trainX , trainY = proj[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "        # testX, testY = proj[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "\n",
    "        svc = SVC(C=3,kernel='rbf')#cora 1.7 citeseer 3\n",
    "        gbdt = GradientBoostingClassifier(n_estimators=8)\n",
    "        rf = RandomForestClassifier()\n",
    "        knn = KNeighborsClassifier()\n",
    "\n",
    "        svc.fit(trainX, trainY)\n",
    "        gbdt.fit(trainX, trainY)\n",
    "        rf.fit(trainX, trainY)\n",
    "        knn.fit(trainX,trainY)\n",
    "\n",
    "        pred_SVC = svc.predict(testX)\n",
    "        pred_GBDT = gbdt.predict(testX)\n",
    "        pred_RF = rf.predict(testX)\n",
    "        pred_KNN = knn.predict(testX)\n",
    "\n",
    "        auc_SVC = metrics.accuracy_score(testY , pred_SVC)\n",
    "        auc_GBDT = metrics.accuracy_score(testY , pred_GBDT)\n",
    "        auc_RF = metrics.accuracy_score(testY , pred_RF)\n",
    "        auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "\n",
    "        print('SVC_auc：%f'%auc_SVC)\n",
    "        print('GBDT_auc：%f'%auc_GBDT)\n",
    "        print('RF_auc：%f'%auc_RF)\n",
    "        print('KNN_auc:%f'%auc_KNN)\n",
    "        acc.append(auc_KNN)\n",
    "\n",
    "    print(np.mean(acc))\n",
    "    print(np.std(acc))\n",
    "    ACC.append(np.mean(acc))\n",
    "    \n",
    "plt.plot(selec,ACC)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('α')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(selec,ACC1)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('α')\n",
    "plt.ylabel('accuracy')\n",
    "plt.savefig('/home/lifuzhen/gcn/sensitivity.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.eval(L)[0,1166]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.isnan(L.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_node_1 = encoder_1.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]]\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "proj = TSNE(random_state=123).fit_transform(encoded_node_1)\n",
    "import seaborn as sns\n",
    "import matplotlib.patheffects as PathEffects\n",
    "y = []\n",
    "hot_y = load_data(dataset)[4][load_data(dataset)[7]].tolist()\n",
    "for i in hot_y:\n",
    "    y.append(i.index(1))\n",
    "y = np.array(y)\n",
    "def scatter(x, colors):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", load_data(dataset)[4].shape[1]))\n",
    "    print(palette)\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "scatter(proj, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "\n",
    "trainX , trainY = encoder_1.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "testX, testY = encoder_1.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "# trainX , trainY = proj[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "# testX, testY = proj[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "svc = SVC(C=1.7,kernel='rbf')#cora 1.7 citeseer 3\n",
    "gbdt = GradientBoostingClassifier(n_estimators=9)\n",
    "rf = RandomForestClassifier()\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "svc.fit(trainX, trainY)\n",
    "gbdt.fit(trainX, trainY)\n",
    "rf.fit(trainX, trainY)\n",
    "knn.fit(trainX,trainY)\n",
    "\n",
    "pred_SVC = svc.predict(testX)\n",
    "pred_GBDT = gbdt.predict(testX)\n",
    "pred_RF = rf.predict(testX)\n",
    "pred_KNN = knn.predict(testX)\n",
    "\n",
    "auc_SVC = metrics.accuracy_score(testY , pred_SVC)\n",
    "auc_GBDT = metrics.accuracy_score(testY , pred_GBDT)\n",
    "auc_RF = metrics.accuracy_score(testY , pred_RF)\n",
    "auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "\n",
    "print('SVC_auc：%f'%auc_SVC)\n",
    "print('GBDT_auc：%f'%auc_GBDT)\n",
    "print('RF_auc：%f'%auc_RF)\n",
    "print('KNN_auc:%f'%auc_KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "for i in range(100):\n",
    "    autoencoder.fit(feature, feature, epochs=10,batch_size=feature.shape[0],shuffle=False,verbose=0)\n",
    "    trainX , trainY = encoder_1.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "    testX, testY = encoder_1.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "    svc = SVC(C=1.7,kernel='rbf')#cora 1.7 citeseer 3\n",
    "    gbdt = GradientBoostingClassifier(n_estimators=8)\n",
    "    rf = RandomForestClassifier()\n",
    "    knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "    svc.fit(trainX, trainY)\n",
    "    gbdt.fit(trainX, trainY)\n",
    "    rf.fit(trainX, trainY)\n",
    "    knn.fit(trainX,trainY)\n",
    "\n",
    "    pred_SVC = svc.predict(testX)\n",
    "    pred_GBDT = gbdt.predict(testX)\n",
    "    pred_RF = rf.predict(testX)\n",
    "    pred_KNN = knn.predict(testX)\n",
    "\n",
    "    auc_SVC = metrics.accuracy_score(testY , pred_SVC)\n",
    "    auc_GBDT = metrics.accuracy_score(testY , pred_GBDT)\n",
    "    auc_RF = metrics.accuracy_score(testY , pred_RF)\n",
    "    auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "    \n",
    "    print('第%d次循环：'%(i+1))\n",
    "    print('SVC_auc：%f'%auc_SVC)\n",
    "    print('GBDT_auc：%f'%auc_GBDT)\n",
    "    print('RF_auc：%f'%auc_RF)\n",
    "    print('KNN_auc:%f'%auc_KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_node_2 = encoder_2.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]]\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "proj = TSNE(random_state=123).fit_transform(encoded_node_2)\n",
    "import seaborn as sns\n",
    "import matplotlib.patheffects as PathEffects\n",
    "y = []\n",
    "hot_y = load_data(dataset)[4][load_data(dataset)[7]].tolist()\n",
    "for i in hot_y:\n",
    "    y.append(i.index(1))\n",
    "y = np.array(y)\n",
    "def scatter(x, colors):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", load_data(dataset)[4].shape[1]))\n",
    "    print(palette)\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "scatter(proj, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "\n",
    "trainX , trainY = encoder_2.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "testX, testY = encoder_2.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "# trainX , trainY = proj[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "# testX, testY = proj[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "\n",
    "svc = SVC(C=3,kernel='rbf')#cora 1.7 citeseer 3\n",
    "gbdt = GradientBoostingClassifier(n_estimators=8)\n",
    "rf = RandomForestClassifier()\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "svc.fit(trainX, trainY)\n",
    "gbdt.fit(trainX, trainY)\n",
    "rf.fit(trainX, trainY)\n",
    "knn.fit(trainX,trainY)\n",
    "\n",
    "pred_SVC = svc.predict(testX)\n",
    "pred_GBDT = gbdt.predict(testX)\n",
    "pred_RF = rf.predict(testX)\n",
    "pred_KNN = knn.predict(testX)\n",
    "\n",
    "auc_SVC = metrics.accuracy_score(testY , pred_SVC)\n",
    "auc_GBDT = metrics.accuracy_score(testY , pred_GBDT)\n",
    "auc_RF = metrics.accuracy_score(testY , pred_RF)\n",
    "auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "\n",
    "print('SVC_auc：%f'%auc_SVC)\n",
    "print('GBDT_auc：%f'%auc_GBDT)\n",
    "print('RF_auc：%f'%auc_RF)\n",
    "print('KNN_auc:%f'%auc_KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "maximum = 0\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "for i in tqdm(range(50)):\n",
    "    autoencoder.fit(feature, feature, epochs=20,batch_size=feature.shape[0],shuffle=False,verbose=0)\n",
    "    trainX , trainY = encoder_2.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "    testX, testY = encoder_2.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "    svc = SVC(C=1.7,kernel='rbf')#cora 1.7 citeseer 3\n",
    "    gbdt = GradientBoostingClassifier(n_estimators=8)\n",
    "    rf = RandomForestClassifier()\n",
    "    knn = KNeighborsClassifier()\n",
    "\n",
    "    svc.fit(trainX, trainY)\n",
    "    gbdt.fit(trainX, trainY)\n",
    "    rf.fit(trainX, trainY)\n",
    "    knn.fit(trainX,trainY)\n",
    "\n",
    "    pred_SVC = svc.predict(testX)\n",
    "    pred_GBDT = gbdt.predict(testX)\n",
    "    pred_RF = rf.predict(testX)\n",
    "    pred_KNN = knn.predict(testX)\n",
    "\n",
    "    auc_SVC = metrics.accuracy_score(testY , pred_SVC)\n",
    "    auc_GBDT = metrics.accuracy_score(testY , pred_GBDT)\n",
    "    auc_RF = metrics.accuracy_score(testY , pred_RF)\n",
    "    auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "    \n",
    "    maximum = max(auc_KNN , maximum)\n",
    "    \n",
    "    print('第%d次循环：'%(i+1))\n",
    "    print('SVC_auc：%f'%auc_SVC)\n",
    "    print('GBDT_auc：%f'%auc_GBDT)\n",
    "    print('RF_auc：%f'%auc_RF)\n",
    "    print('KNN_auc:%f'%auc_KNN)\n",
    "    \n",
    "print('最高准确率为:%f'%maximum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_node_3 = encoder_3.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]]\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "proj = TSNE(random_state=123).fit_transform(encoded_node_3)\n",
    "import seaborn as sns\n",
    "import matplotlib.patheffects as PathEffects\n",
    "y = []\n",
    "hot_y = load_data(dataset)[4][load_data(dataset)[7]].tolist()\n",
    "for i in hot_y:\n",
    "    y.append(i.index(1))\n",
    "y = np.array(y)\n",
    "def scatter(x, colors):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", load_data(dataset)[4].shape[1]))\n",
    "    print(palette)\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "scatter(proj, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "\n",
    "# trainX , trainY = encoder_3.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "# testX, testY = encoder_3.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "trainX , trainY = proj[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "testX, testY = proj[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "\n",
    "\n",
    "svc = SVC(C=3,kernel='rbf')#cora 1.7 citeseer 3\n",
    "gbdt = GradientBoostingClassifier(n_estimators=8)\n",
    "rf = RandomForestClassifier()\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "svc.fit(trainX, trainY)\n",
    "gbdt.fit(trainX, trainY)\n",
    "rf.fit(trainX, trainY)\n",
    "knn.fit(trainX,trainY)\n",
    "\n",
    "pred_SVC = svc.predict(testX)\n",
    "pred_GBDT = gbdt.predict(testX)\n",
    "pred_RF = rf.predict(testX)\n",
    "pred_KNN = knn.predict(testX)\n",
    "\n",
    "auc_SVC = metrics.accuracy_score(testY , pred_SVC)\n",
    "auc_GBDT = metrics.accuracy_score(testY , pred_GBDT)\n",
    "auc_RF = metrics.accuracy_score(testY , pred_RF)\n",
    "auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "\n",
    "print('SVC_auc：%f'%auc_SVC)\n",
    "print('GBDT_auc：%f'%auc_GBDT)\n",
    "print('RF_auc：%f'%auc_RF)\n",
    "print('KNN_auc:%f'%auc_KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "for i in range(100):\n",
    "    autoencoder.fit(feature, feature, epochs=10,batch_size=feature.shape[0],shuffle=False,verbose=0)\n",
    "    trainX , trainY = encoder_3.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "    testX, testY = encoder_3.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "    svc = SVC(C=1.7,kernel='rbf')#cora 1.7 citeseer 3\n",
    "    gbdt = GradientBoostingClassifier(n_estimators=8)\n",
    "    rf = RandomForestClassifier()\n",
    "    knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "    svc.fit(trainX, trainY)\n",
    "    gbdt.fit(trainX, trainY)\n",
    "    rf.fit(trainX, trainY)\n",
    "    knn.fit(trainX,trainY)\n",
    "\n",
    "    pred_SVC = svc.predict(testX)\n",
    "    pred_GBDT = gbdt.predict(testX)\n",
    "    pred_RF = rf.predict(testX)\n",
    "    pred_KNN = knn.predict(testX)\n",
    "\n",
    "    auc_SVC = metrics.accuracy_score(testY , pred_SVC)\n",
    "    auc_GBDT = metrics.accuracy_score(testY , pred_GBDT)\n",
    "    auc_RF = metrics.accuracy_score(testY , pred_RF)\n",
    "    auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "    \n",
    "    print('第%d次循环：'%(i+1))\n",
    "    print('SVC_auc：%f'%auc_SVC)\n",
    "    print('GBDT_auc：%f'%auc_GBDT)\n",
    "    print('RF_auc：%f'%auc_RF)\n",
    "    print('KNN_auc:%f'%auc_KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_node_1 = decoder_1.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]]\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "proj = TSNE(random_state=123).fit_transform(decoded_node_1)\n",
    "import seaborn as sns\n",
    "import matplotlib.patheffects as PathEffects\n",
    "y = []\n",
    "hot_y = load_data(dataset)[4][load_data(dataset)[7]].tolist()\n",
    "for i in hot_y:\n",
    "    y.append(i.index(1))\n",
    "y = np.array(y)\n",
    "def scatter(x, colors):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", load_data(dataset)[4].shape[1]))\n",
    "    print(palette)\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "scatter(proj, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "\n",
    "trainX , trainY = decoder_1.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "testX, testY = decoder_1.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "# trainX , trainY = proj[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "# testX, testY = proj[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "\n",
    "svc = SVC(C=1.7,kernel='rbf')#cora 1.7 citeseer 3\n",
    "gbdt = GradientBoostingClassifier(n_estimators=8)\n",
    "rf = RandomForestClassifier()\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "svc.fit(trainX, trainY)\n",
    "gbdt.fit(trainX, trainY)\n",
    "rf.fit(trainX, trainY)\n",
    "knn.fit(trainX,trainY)\n",
    "\n",
    "pred_SVC = svc.predict(testX)\n",
    "pred_GBDT = gbdt.predict(testX)\n",
    "pred_RF = rf.predict(testX)\n",
    "pred_KNN = knn.predict(testX)\n",
    "\n",
    "auc_SVC = metrics.accuracy_score(testY , pred_SVC)\n",
    "auc_GBDT = metrics.accuracy_score(testY , pred_GBDT)\n",
    "auc_RF = metrics.accuracy_score(testY , pred_RF)\n",
    "auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "\n",
    "print('SVC_auc：%f'%auc_SVC)\n",
    "print('GBDT_auc：%f'%auc_GBDT)\n",
    "print('RF_auc：%f'%auc_RF)\n",
    "print('KNN_auc:%f'%auc_KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "for i in range(100):\n",
    "    autoencoder.fit(feature, feature, epochs=10,batch_size=feature.shape[0],shuffle=False,verbose=0)\n",
    "    trainX , trainY = decoder_1.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "    testX, testY = decoder_1.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "    svc = SVC(C=1.7,kernel='rbf')#cora 1.7 citeseer 3\n",
    "    gbdt = GradientBoostingClassifier(n_estimators=8)\n",
    "    rf = RandomForestClassifier()\n",
    "    knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "    svc.fit(trainX, trainY)\n",
    "    gbdt.fit(trainX, trainY)\n",
    "    rf.fit(trainX, trainY)\n",
    "    knn.fit(trainX,trainY)\n",
    "\n",
    "    pred_SVC = svc.predict(testX)\n",
    "    pred_GBDT = gbdt.predict(testX)\n",
    "    pred_RF = rf.predict(testX)\n",
    "    pred_KNN = knn.predict(testX)\n",
    "\n",
    "    auc_SVC = metrics.accuracy_score(testY , pred_SVC)\n",
    "    auc_GBDT = metrics.accuracy_score(testY , pred_GBDT)\n",
    "    auc_RF = metrics.accuracy_score(testY , pred_RF)\n",
    "    auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "    \n",
    "    print('第%d次循环：'%(i+1))\n",
    "    print('SVC_auc：%f'%auc_SVC)\n",
    "    print('GBDT_auc：%f'%auc_GBDT)\n",
    "    print('RF_auc：%f'%auc_RF)\n",
    "    print('KNN_auc:%f'%auc_KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_node_2 = decoder_2.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]]\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "proj = TSNE(random_state=123).fit_transform(decoded_node_2)\n",
    "import seaborn as sns\n",
    "import matplotlib.patheffects as PathEffects\n",
    "y = []\n",
    "hot_y = load_data(dataset)[4][load_data(dataset)[7]].tolist()\n",
    "for i in hot_y:\n",
    "    y.append(i.index(1))\n",
    "y = np.array(y)\n",
    "def scatter(x, colors):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", load_data(dataset)[4].shape[1]))\n",
    "    print(palette)\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "scatter(proj, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "\n",
    "trainX , trainY = proj[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "testX, testY = proj[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "svc = SVC(C=1.7,kernel='rbf')#cora 1.7 citeseer 3\n",
    "gbdt = GradientBoostingClassifier(n_estimators=8)\n",
    "rf = RandomForestClassifier()\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "svc.fit(trainX, trainY)\n",
    "gbdt.fit(trainX, trainY)\n",
    "rf.fit(trainX, trainY)\n",
    "knn.fit(trainX,trainY)\n",
    "\n",
    "pred_SVC = svc.predict(testX)\n",
    "pred_GBDT = gbdt.predict(testX)\n",
    "pred_RF = rf.predict(testX)\n",
    "pred_KNN = knn.predict(testX)\n",
    "\n",
    "auc_SVC = metrics.accuracy_score(testY , pred_SVC)\n",
    "auc_GBDT = metrics.accuracy_score(testY , pred_GBDT)\n",
    "auc_RF = metrics.accuracy_score(testY , pred_RF)\n",
    "auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "\n",
    "print('SVC_auc：%f'%auc_SVC)\n",
    "print('GBDT_auc：%f'%auc_GBDT)\n",
    "print('RF_auc：%f'%auc_RF)\n",
    "print('KNN_auc:%f'%auc_KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "for i in range(100):\n",
    "    autoencoder.fit(feature, feature, epochs=10,batch_size=feature.shape[0],shuffle=False,verbose=0)\n",
    "    trainX , trainY = decoder_2.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "    testX, testY = decoder_2.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "    svc = SVC(C=1.7,kernel='rbf')#cora 1.7 citeseer 3\n",
    "    gbdt = GradientBoostingClassifier(n_estimators=8)\n",
    "    rf = RandomForestClassifier()\n",
    "    knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "    svc.fit(trainX, trainY)\n",
    "    gbdt.fit(trainX, trainY)\n",
    "    rf.fit(trainX, trainY)\n",
    "    knn.fit(trainX,trainY)\n",
    "\n",
    "    pred_SVC = svc.predict(testX)\n",
    "    pred_GBDT = gbdt.predict(testX)\n",
    "    pred_RF = rf.predict(testX)\n",
    "    pred_KNN = knn.predict(testX)\n",
    "\n",
    "    auc_SVC = metrics.accuracy_score(testY , pred_SVC)\n",
    "    auc_GBDT = metrics.accuracy_score(testY , pred_GBDT)\n",
    "    auc_RF = metrics.accuracy_score(testY , pred_RF)\n",
    "    auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "    \n",
    "    print('第%d次循环：'%(i+1))\n",
    "    print('SVC_auc：%f'%auc_SVC)\n",
    "    print('GBDT_auc：%f'%auc_GBDT)\n",
    "    print('RF_auc：%f'%auc_RF)\n",
    "    print('KNN_auc:%f'%auc_KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "for i in range(100):\n",
    "    autoencoder.fit(feature, feature, epochs=10,batch_size=feature.shape[0],shuffle=False,verbose=0)\n",
    "    trainX_1 , trainY_1 = encoder_1.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "    testX_1, testY_1 = encoder_1.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "    trainX_2 , trainY_2 = encoder_2.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "    testX_2, testY_2 = encoder_2.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "    trainX_3 , trainY_3 = decoder_1.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "    testX_3, testY_3 = decoder_1.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "    \n",
    "    svc_1 = SVC(C=1.7,kernel='rbf')#cora 1.7 citeseer 3\n",
    "    svc_2 = SVC(C=1.7,kernel='rbf')\n",
    "    svc_3 = SVC(C=1.7,kernel='rbf')\n",
    "    \n",
    "    svc_1.fit(trainX_1, trainY_1)\n",
    "    svc_2.fit(trainX_2, trainY_2)\n",
    "    svc_3.fit(trainX_3, trainY_3)\n",
    "\n",
    "    pred_SVC_1 = svc_1.predict(testX_1)\n",
    "    pred_SVC_2 = svc_2.predict(testX_2)\n",
    "    pred_SVC_3 = svc_3.predict(testX_3)\n",
    "\n",
    "    auc_SVC_1 = metrics.accuracy_score(testY_1 , pred_SVC_1)\n",
    "    auc_SVC_2 = metrics.accuracy_score(testY_2 , pred_SVC_2)\n",
    "    auc_SVC_3 = metrics.accuracy_score(testY_3 , pred_SVC_3)\n",
    "    \n",
    "    print('第%d次循环：'%(i+1))\n",
    "    print('SVC_auc_1：%f'%auc_SVC_1)\n",
    "    print('SVC_auc_2：%f'%auc_SVC_2)\n",
    "    print('SVC_auc_3：%f'%auc_SVC_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 没有图传递"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense  ,Dot ,Lambda ,BatchNormalization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model  \n",
    "from keras.datasets import mnist  \n",
    "from keras import backend as K\n",
    "import numpy as np  \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt  \n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "# 指定第一块GPU可用 \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)\n",
    "acc = []\n",
    "for n in range(1):\n",
    "    dataset = 'cora'  #citeseer   cora  pubmed\n",
    "\n",
    "    feature = load_data(dataset)[1].toarray()\n",
    "    # original_train, original_test = feature[load_data(dataset)[5]], feature[load_data(dataset)[7]]\n",
    "    # x_train, x_test = feature[load_data(dataset)[5]], feature[load_data(dataset)[7]]\n",
    "    y_train = load_data(dataset)[2][load_data(dataset)[5]].tolist()\n",
    "    t = 1\n",
    "    ####citeseer########\n",
    "    # encoding_dim_1 = 2048\n",
    "    # encoding_dim_2 = 512\n",
    "    ####cora #######\n",
    "    encoding_dim_1 = 2048\n",
    "    encoding_dim_2 = 512\n",
    "    ######pubmed#####\n",
    "    # encoding_dim_1 = 8\n",
    "    # encoding_dim_2 = 8\n",
    "\n",
    "    order_00 = Input(shape=(feature.shape[1],)) \n",
    "\n",
    "    # def graph_convex(x):\n",
    "    #     x1 = x\n",
    "    #     return x1\n",
    "\n",
    "\n",
    "    # order_11 = Lambda(function=graph_convex)(order_00)\n",
    "    encoded_11 = Dense(encoding_dim_1, activation='tanh')(order_00) \n",
    "    encoded_11 = BatchNormalization(axis = -1)(encoded_11)\n",
    "\n",
    "    # order_22 = Lambda(function= graph_convex)(encoded_11)\n",
    "    encoded_22 = Dense(encoding_dim_2, activation='tanh')(encoded_11)  \n",
    "    encoded_22 = BatchNormalization(axis = -1)(encoded_22)\n",
    "\n",
    "    # order_33 = Lambda(function= graph_convex)(encoded_22)\n",
    "    decoded_11 = Dense(encoding_dim_1, activation='tanh')(encoded_22)  \n",
    "    decoded_11 = BatchNormalization(axis = -1)(decoded_11)\n",
    "\n",
    "    # order_44 = Lambda(function= graph_convex)(decoded_11)\n",
    "    decoded_22 = Dense(feature.shape[1], activation='tanh')(decoded_11)  \n",
    "\n",
    "    autoencoder_1 = Model(inputs=order_00, outputs=decoded_22)  \n",
    "    encoder_11 = Model(inputs=order_00, outputs=encoded_11)    \n",
    "    encoder_22 = Model(inputs=order_00, outputs=encoded_22)\n",
    "    decoder_11 = Model(inputs=order_00, outputs=decoded_11)\n",
    "\n",
    "    autoencoder_1.compile(optimizer='Adam', loss='binary_crossentropy')\n",
    "    autoencoder_1.fit(feature, feature, epochs=300,batch_size=feature.shape[0],shuffle=False)\n",
    "\n",
    "\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "    from sklearn.ensemble.forest import RandomForestClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn import metrics\n",
    "    def onehot2index(onehot):\n",
    "        index = []\n",
    "        onehot = onehot.tolist()\n",
    "        for i in onehot:\n",
    "            index.append(i.index(1))\n",
    "        return np.array(index)\n",
    "    trainX , trainY = encoder_22.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "    testX, testY = encoder_22.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "    SVC = SVC(C=1,kernel='rbf')\n",
    "    GBDT = GradientBoostingClassifier(n_estimators=8)\n",
    "    RF = RandomForestClassifier()\n",
    "    KNN = KNeighborsClassifier()\n",
    "\n",
    "    SVC.fit(trainX, trainY)\n",
    "    GBDT.fit(trainX, trainY)\n",
    "    RF.fit(trainX, trainY)\n",
    "    KNN.fit(trainX,trainY)\n",
    "\n",
    "    pred_SVC = SVC.predict(testX)\n",
    "    pred_GBDT = GBDT.predict(testX)\n",
    "    pred_RF = RF.predict(testX)\n",
    "    pred_KNN = KNN.predict(testX)\n",
    "\n",
    "    auc_SVC = metrics.accuracy_score(testY , pred_SVC)\n",
    "    auc_GBDT = metrics.accuracy_score(testY , pred_GBDT)\n",
    "    auc_RF = metrics.accuracy_score(testY , pred_RF)\n",
    "    auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "    \n",
    "    encoded_node_1 = encoder_22.predict(feature,batch_size=feature.shape[0])\n",
    "\n",
    "    proj2 = TSNE(random_state=123,perplexity=30,early_exaggeration=12,init='random').fit_transform(encoded_node_1)\n",
    "    \n",
    "    y = onehot2index(load_data(dataset)[8])\n",
    "#     plt.subplot(133)\n",
    "    scatter(proj2, y)\n",
    "#     plt.savefig('/home/lifuzhen/gcn/visulization.png')\n",
    "    plt.show()\n",
    "    \n",
    "    print('SVC_auc：%f'%auc_SVC)\n",
    "    print('GBDT_auc：%f'%auc_GBDT)\n",
    "    print('RF_auc：%f'%auc_RF)\n",
    "    print('KNN_auc:%f'%auc_KNN)\n",
    "    acc.append(auc_KNN)\n",
    "    \n",
    "print(np.mean(acc))\n",
    "print(np.std(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(proj1,y)\n",
    "plt.savefig('/home/lifuzhen/gcn/proj1.png')\n",
    "scatter(proj2,y)\n",
    "plt.savefig('/home/lifuzhen/gcn/proj2.png')\n",
    "scatter(proj3,y)\n",
    "plt.savefig('/home/lifuzhen/gcn/proj3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一层表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_node_11 = encoder_11.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]]\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "proj = TSNE(random_state=123).fit_transform(encoded_node_11)\n",
    "import seaborn as sns\n",
    "import matplotlib.patheffects as PathEffects\n",
    "y = []\n",
    "hot_y = load_data(dataset)[4][load_data(dataset)[7]].tolist()\n",
    "for i in hot_y:\n",
    "    y.append(i.index(1))\n",
    "y = np.array(y)\n",
    "def scatter(x, colors):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", load_data(dataset)[4].shape[1]))\n",
    "    print(palette)\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "scatter(proj, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "trainX , trainY = encoder_11.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "testX, testY = encoder_11.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "SVC = SVC(C=1,kernel='rbf')\n",
    "GBDT = GradientBoostingClassifier(n_estimators=8)\n",
    "RF = RandomForestClassifier()\n",
    "KNN = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "SVC.fit(trainX, trainY)\n",
    "GBDT.fit(trainX, trainY)\n",
    "RF.fit(trainX, trainY)\n",
    "KNN.fit(trainX,trainY)\n",
    "\n",
    "pred_SVC = SVC.predict(testX)\n",
    "pred_GBDT = GBDT.predict(testX)\n",
    "pred_RF = RF.predict(testX)\n",
    "pred_KNN = KNN.predict(testX)\n",
    "\n",
    "auc_SVC = metrics.accuracy_score(testY , pred_SVC)\n",
    "auc_GBDT = metrics.accuracy_score(testY , pred_GBDT)\n",
    "auc_RF = metrics.accuracy_score(testY , pred_RF)\n",
    "auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "\n",
    "print('SVC_auc：%f'%auc_SVC)\n",
    "print('GBDT_auc：%f'%auc_GBDT)\n",
    "print('RF_auc：%f'%auc_RF)\n",
    "print('KNN_auc:%f'%auc_KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二层表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_node_22 = encoder_22.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]]\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "proj = TSNE(random_state=123).fit_transform(encoded_node_22)\n",
    "import seaborn as sns\n",
    "import matplotlib.patheffects as PathEffects\n",
    "y = []\n",
    "hot_y = load_data(dataset)[4][load_data(dataset)[7]].tolist()\n",
    "for i in hot_y:\n",
    "    y.append(i.index(1))\n",
    "y = np.array(y)\n",
    "def scatter(x, colors):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", load_data(dataset)[4].shape[1]))\n",
    "    print(palette)\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "scatter(proj, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "trainX , trainY = encoder_22.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "testX, testY = encoder_22.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "SVC = SVC(C=1,kernel='rbf')\n",
    "GBDT = GradientBoostingClassifier(n_estimators=8)\n",
    "RF = RandomForestClassifier()\n",
    "KNN = KNeighborsClassifier()\n",
    "\n",
    "SVC.fit(trainX, trainY)\n",
    "GBDT.fit(trainX, trainY)\n",
    "RF.fit(trainX, trainY)\n",
    "KNN.fit(trainX,trainY)\n",
    "\n",
    "pred_SVC = SVC.predict(testX)\n",
    "pred_GBDT = GBDT.predict(testX)\n",
    "pred_RF = RF.predict(testX)\n",
    "pred_KNN = KNN.predict(testX)\n",
    "\n",
    "auc_SVC = metrics.accuracy_score(testY , pred_SVC)\n",
    "auc_GBDT = metrics.accuracy_score(testY , pred_GBDT)\n",
    "auc_RF = metrics.accuracy_score(testY , pred_RF)\n",
    "auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "\n",
    "print('SVC_auc：%f'%auc_SVC)\n",
    "print('GBDT_auc：%f'%auc_GBDT)\n",
    "print('RF_auc：%f'%auc_RF)\n",
    "print('KNN_auc:%f'%auc_KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三层表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_node_11 = decoder_11.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]]\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "proj = TSNE(random_state=123).fit_transform(decoded_node_11)\n",
    "import seaborn as sns\n",
    "import matplotlib.patheffects as PathEffects\n",
    "y = []\n",
    "hot_y = load_data(dataset)[4][load_data(dataset)[7]].tolist()\n",
    "for i in hot_y:\n",
    "    y.append(i.index(1))\n",
    "y = np.array(y)\n",
    "def scatter(x, colors):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", load_data(dataset)[4].shape[1]))\n",
    "    print(palette)\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "scatter(proj, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "trainX , trainY = decoder_11.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "testX, testY = decoder_11.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "SVC = SVC(C=1,kernel='rbf')\n",
    "GBDT = GradientBoostingClassifier(n_estimators=8)\n",
    "RF = RandomForestClassifier()\n",
    "KNN = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "SVC.fit(trainX, trainY)\n",
    "GBDT.fit(trainX, trainY)\n",
    "RF.fit(trainX, trainY)\n",
    "KNN.fit(trainX,trainY)\n",
    "\n",
    "pred_SVC = SVC.predict(testX)\n",
    "pred_GBDT = GBDT.predict(testX)\n",
    "pred_RF = RF.predict(testX)\n",
    "pred_KNN = KNN.predict(testX)\n",
    "\n",
    "auc_SVC = metrics.accuracy_score(testY , pred_SVC)\n",
    "auc_GBDT = metrics.accuracy_score(testY , pred_GBDT)\n",
    "auc_RF = metrics.accuracy_score(testY , pred_RF)\n",
    "auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "\n",
    "print('SVC_auc：%f'%auc_SVC)\n",
    "print('GBDT_auc：%f'%auc_GBDT)\n",
    "print('RF_auc：%f'%auc_RF)\n",
    "print('KNN_auc:%f'%auc_KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encoded_node = encoder.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]]\n",
    "encoded_node = encoder.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]]\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "proj = TSNE(random_state=123).fit_transform(encoded_node)\n",
    "import seaborn as sns\n",
    "import matplotlib.patheffects as PathEffects\n",
    "y = []\n",
    "# hot_y = load_data(dataset)[2][load_data(dataset)[5]].tolist()\n",
    "hot_y = load_data(dataset)[4][load_data(dataset)[7]].tolist()\n",
    "for i in hot_y:\n",
    "    y.append(i.index(1))\n",
    "y = np.array(y)\n",
    "def scatter(x, colors):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", load_data(dataset)[4].shape[1]))\n",
    "\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "scatter(proj, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### from keras.layers import Input, Dense  ,Dot ,Lambda\n",
    "from keras.models import Model  \n",
    "from keras.datasets import mnist  \n",
    "from keras import backend as K\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "\n",
    "adj = load_data('citeseer')[0]\n",
    "adj_normalize = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "feature = load_data('citeseer')[1]\n",
    "x_train, x_test = adj_normalize.dot(feature)[load_data('citeseer')[5]], adj_normalize.dot(feature)[load_data('citeseer')[7]]\n",
    "y_train = load_data('citeseer')[2][load_data('citeseer')[5]].tolist()\n",
    "sample_num = 5 ##随机采样数\n",
    "margin = 1  ##类间距\n",
    "encoding_dim_1 = 1024\n",
    "encoding_dim_2 = 128\n",
    "input_node = Input(shape=(3703,))  \n",
    "a_input_node = Input(shape=(3703,)) ##anchor\n",
    "p_input_node = Input(shape=(3703,))  ##positive\n",
    "n_input_node = Input(shape=(3703,))  ##negative\n",
    "\n",
    "encoded_1 = Dense(encoding_dim_1, activation='relu')(input_node)  \n",
    "encoded_2 = Dense(encoding_dim_2, activation='sigmoid')(encoded_1)  \n",
    "decoded = Dense(3703, activation='sigmoid')(encoded_2)  \n",
    "\n",
    "autoencoder = Model(inputs=input_node, outputs=decoded)  \n",
    "encoder = Model(inputs=input_node, outputs=encoded_2)    \n",
    "\n",
    "encoded_input = Input(shape=(encoding_dim_2,))  \n",
    "decoder_layer = autoencoder.layers[-1]  \n",
    "decoder = Model(inputs=encoded_input, outputs=decoder_layer(encoded_input)) \n",
    "a_encoded = autoencoder.layers[-2](autoencoder.layers[-3](a_input_node))\n",
    "p_encoded = autoencoder.layers[-2](autoencoder.layers[-3](p_input_node))\n",
    "n_encoded = autoencoder.layers[-2](autoencoder.layers[-3](n_input_node))\n",
    " \n",
    "triplet = Lambda(lambda x: K.relu(K.sum(K.square(x[0]-x[1]),1,keepdims=True) - K.sum(K.square(x[0]-x[2]),1,keepdims=True)+ margin))([a_encoded,p_encoded,n_encoded])\n",
    "triplet_model = Model(inputs= [a_input_node,p_input_node,n_input_node], outputs= triplet)\n",
    "triplet_model.compile(optimizer='Adam', loss=lambda y_true,y_pred: y_pred)\n",
    "autoencoder.compile(optimizer='Adam', loss='mse')\n",
    "for m in tqdm(range(0,10)):\n",
    "    for j in range(0,6):\n",
    "        \n",
    "        C1 = []  ##target and positive\n",
    "        C2 = []  ##negative\n",
    "        C = [C1,C2]\n",
    "        count = 0\n",
    "        for i in y_train:\n",
    "            if i.index(1) == j:\n",
    "                C1.append(x_train[count])\n",
    "            if i.index(1) != j:\n",
    "                C2.append(x_train[count])\n",
    "            count += 1\n",
    "\n",
    "#         print(x_train.shape)  \n",
    "#         print(x_test.shape)  \n",
    "  \n",
    "\n",
    "#         autoencoder.fit(x_train, x_train, epochs=10, batch_size=32,   \n",
    "#                         shuffle=True, validation_data=(x_test, x_test))\n",
    "        for n in range(5):\n",
    "#             triplet_model.fit([sp.vstack(sample(C1,sample_num)),sp.vstack(sample(C1,sample_num)),sp.vstack(sample(C2,sample_num))],\n",
    "#                               np.zeros((sample_num,1)), epochs=10, batch_size=10,shuffle=True)\n",
    "            triplet_model.fit([sp.vstack(sample(C1,sample_num)),sp.vstack(sample(C1,sample_num)),sp.vstack(sample(C2,sample_num))],\n",
    "                              np.zeros((sample_num,1)), epochs=10)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "          \n",
    "\n",
    "encoded_imgs = encoder.predict(x_test)  \n",
    "decoded_imgs = decoder.predict(encoded_imgs)  \n",
    "\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "proj = TSNE(random_state=123).fit_transform(encoded_imgs)\n",
    "import seaborn as sns\n",
    "import matplotlib.patheffects as PathEffects\n",
    "y = []\n",
    "hot_y = load_data('citeseer')[4][load_data('citeseer')[7]].tolist()\n",
    "for i in hot_y:\n",
    "    y.append(i.index(1))\n",
    "y = np.array(y)\n",
    "def scatter(x, colors):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", 6))\n",
    "    print(palette)\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "scatter(proj, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### from keras.layers import Input, Dense  ,Dot ,Lambda\n",
    "from keras.models import Model  \n",
    "from keras.datasets import mnist  \n",
    "from keras import backend as K\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "\n",
    "adj = load_data('citeseer')[0]\n",
    "adj_normalize = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "feature = load_data('citeseer')[1]\n",
    "x_train, x_test = adj_normalize.dot(adj_normalize).dot(feature)[load_data('citeseer')[5]], adj_normalize.dot(adj_normalize).dot(feature)[load_data('citeseer')[7]]\n",
    "y_train = load_data('citeseer')[2][load_data('citeseer')[5]].tolist()\n",
    "sample_num = 4 ##随机采样数\n",
    "margin = 1  ##类间距\n",
    "encoding_dim = 1024\n",
    "lamb = 0.9\n",
    "a_input_node = Input(shape=(3703,)) ##anchor\n",
    "p_input_node = Input(shape=(3703,))  ##positive\n",
    "n_input_node = Input(shape=(3703,))  ##negative\n",
    "\n",
    "a_encoded = Dense(encoding_dim, activation='sigmoid')(a_input_node)   \n",
    "p_encoded = Dense(encoding_dim, activation='sigmoid')(p_input_node) \n",
    "n_encoded = Dense(encoding_dim, activation='sigmoid')(n_input_node) \n",
    "a_decoded = Dense(3703, activation='sigmoid')(a_encoded)  \n",
    "\n",
    "triplet = Lambda(lambda x: lamb * K.mean(K.sum(K.square(x[4]-x[3]))) + (1 - lamb) * K.maximum(K.sum(K.square(x[0]-x[1])) \\\n",
    "                    - K.sum(K.square(x[0]-x[2]))+ margin,0))([a_encoded,p_encoded,n_encoded,a_decoded,a_input_node])\n",
    "autoencoder = Model(inputs=[a_input_node,p_input_node,n_input_node], outputs=triplet)  \n",
    "encoder = Model(inputs=a_input_node, outputs=a_encoded)    \n",
    "\n",
    "encoded_input = Input(shape=(encoding_dim,))  \n",
    "decoder_layer = autoencoder.layers[-1]  \n",
    "decoder = Model(inputs=encoded_input, outputs=decoder_layer(encoded_input)) \n",
    "autoencoder.compile(optimizer='Adam', loss=lambda y_true,y_pred: y_pred)\n",
    "C0 = []\n",
    "C1 = []  ##target and positive\n",
    "C2 = []  ##negative\n",
    "C3 = []\n",
    "C4 = []\n",
    "C5 = []\n",
    "C = [C0,C1,C2,C3,C4,C5]\n",
    "for j in range(0,6):\n",
    "   \n",
    "    count = 0\n",
    "    for i in y_train:\n",
    "        if i.index(1) == j:\n",
    "            C[j].append(x_train[count])\n",
    "        count += 1\n",
    "\n",
    "for m in tqdm(range(0,10)):\n",
    "    for n in range(1):\n",
    "        for j in range(0,6):\n",
    "            for k in range(0,6):\n",
    "                if j != k:\n",
    "                    a = sp.vstack(sample(C[j],sample_num))\n",
    "                    p = sp.vstack(sample(C[j],sample_num))\n",
    "                    n = sp.vstack(sample(C[k],sample_num))\n",
    "    #                 triplet_model.fit([sp.vstack(sample(C[j],sample_num)),sp.vstack(sample(C[j],sample_num)),sp.vstack(sample(C[k],sample_num))],\n",
    "    #                               np.zeros((sample_num,1)), epochs=20, batch_size=2,shuffle=True)\n",
    "                    autoencoder.fit([a,p,n],\n",
    "                                  np.zeros((sample_num,1)), epochs=20, batch_size=2,shuffle=True)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "          \n",
    "\n",
    "    encoded_imgs = encoder.predict(x_test)  \n",
    "    decoded_imgs = decoder.predict(encoded_imgs)  \n",
    "\n",
    "\n",
    "\n",
    "    from sklearn.manifold import TSNE\n",
    "    proj = TSNE(random_state=1).fit_transform(encoded_imgs)\n",
    "    import seaborn as sns\n",
    "    import matplotlib.patheffects as PathEffects\n",
    "    y = []\n",
    "    hot_y = load_data('citeseer')[4][load_data('citeseer')[7]].tolist()\n",
    "    for i in hot_y:\n",
    "        y.append(i.index(1))\n",
    "    y = np.array(y)\n",
    "    def scatter(x, colors):\n",
    "        # We choose a color palette with seaborn.\n",
    "        palette = np.array(sns.color_palette(\"hls\", 6))\n",
    "        print(y.shape)\n",
    "        # We create a scatter plot.\n",
    "        f = plt.figure(figsize=(8, 8))\n",
    "        ax = plt.subplot(aspect='equal')\n",
    "        sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                        c=palette[colors.astype(np.int)])\n",
    "        plt.xlim(-25, 25)\n",
    "        plt.ylim(-25, 25)\n",
    "        ax.axis('off')\n",
    "        ax.axis('tight')\n",
    "    scatter(proj, y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs = encoder.predict(x_train)  \n",
    "decoded_imgs = decoder.predict(encoded_imgs)  \n",
    "\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "proj = TSNE(random_state=1).fit_transform(encoded_imgs)\n",
    "import seaborn as sns\n",
    "import matplotlib.patheffects as PathEffects\n",
    "y = []\n",
    "hot_y = load_data('citeseer')[2][load_data('citeseer')[5]].tolist()\n",
    "for i in hot_y:\n",
    "    y.append(i.index(1))\n",
    "y = np.array(y)\n",
    "def scatter(x, colors):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", 6))\n",
    "\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "scatter(proj, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense  ,Dot\n",
    "from keras.models import Model  \n",
    "from keras.datasets import mnist  \n",
    "from keras import backend as K\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "adj = load_data('citeseer')[0]\n",
    "adj_normalize = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "feature = load_data('citeseer')[1]\n",
    "x_train, x_test = adj_normalize.dot(feature)[load_data('citeseer')[5]], adj_normalize.dot(feature)[load_data('citeseer')[7]]\n",
    "\n",
    "print(x_train.shape)  \n",
    "print(x_test.shape)  \n",
    "\n",
    "margin = 1\n",
    "encoding_dim = 32  \n",
    "input_node = Input(shape=(3703,))  \n",
    "a_input_node = Input(shape=(3703,)) \n",
    "p_input_node = Input(shape=(3703,)) \n",
    "n_input_node = Input(shape=(3703,)) \n",
    "\n",
    "encoded = Dense(encoding_dim, activation='sigmoid')(input_node)  \n",
    "decoded = Dense(3703, activation='sigmoid')(encoded)  \n",
    "\n",
    "autoencoder = Model(inputs=input_node, outputs=decoded)  \n",
    "encoder = Model(inputs=input_node, outputs=encoded)    \n",
    "\n",
    "encoded_input = Input(shape=(encoding_dim,))  \n",
    "decoder_layer = autoencoder.layers[-1]  \n",
    "a_encoded = autoencoder.layers[-2](a_input_node)\n",
    "p_encoded = autoencoder.layers[-2](p_input_node)\n",
    "n_encoded = autoencoder.layers[-2](n_input_node)\n",
    "triplet = K.sum(K.square(a_encoded-p_encoded)) - K.sum(K.square(a_encoded-n_encoded))+ margin\n",
    "\n",
    "decoder = Model(inputs=encoded_input, outputs=decoder_layer(encoded_input))  \n",
    "\n",
    "\n",
    "autoencoder.compile(optimizer='Adam', loss='mse')  \n",
    "  \n",
    "autoencoder.fit(x_train, x_train, epochs=50, batch_size=16,   \n",
    "                shuffle=True, validation_data=(x_test, x_test))  \n",
    "  \n",
    "encoded_imgs = encoder.predict(x_test)  \n",
    "decoded_imgs = decoder.predict(encoded_imgs)  \n",
    "  \n",
    "from sklearn.manifold import TSNE\n",
    "proj = TSNE(random_state=1).fit_transform(encoded_imgs)\n",
    "import seaborn as sns\n",
    "import matplotlib.patheffects as PathEffects\n",
    "y = []\n",
    "hot_y = load_data('citeseer')[4][load_data('citeseer')[7]].tolist()\n",
    "for i in hot_y:\n",
    "    y.append(i.index(1))\n",
    "y = np.array(y)\n",
    "def scatter(x, colors):\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", 10))\n",
    "\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # We add the labels for each digit.\n",
    "    txts = []\n",
    "#     for i in range(10):\n",
    "#         # Position of each label.\n",
    "#         xtext, ytext = np.median(x[colors == i, :], axis=0)\n",
    "#         txt = ax.text(xtext, ytext, str(i), fontsize=24)\n",
    "#         txt.set_path_effects([\n",
    "#             PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
    "#             PathEffects.Normal()])\n",
    "#         txts.append(txt)\n",
    "\n",
    "#     return f, ax, sc, txts\n",
    "scatter(proj, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "\n",
    "dataset = 'pubmed'  #citeseer   cora  pubmed\n",
    "dim = 256\n",
    "feature = load_data(dataset)[1].toarray()\n",
    "# #####逼近式\n",
    "# alpha = 0.99\n",
    "# adj = load_data(dataset)[0]\n",
    "# adj_normalize = normalize_adj(adj)\n",
    "# adj_normalize = np.linalg.inv(np.eye(adj.shape[0]) - 0.9*adj_normalize.toarray())\n",
    "# # adj_normalize = np.eye(adj.shape[0]) + alpha*adj_normalize + alpha**2*np.dot(adj_normalize,adj_normalize.T)\n",
    "\n",
    "embeding = TruncatedSVD(n_components=dim).fit_transform(feature)\n",
    "\n",
    "trainX , trainY = embeding[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "testX, testY = embeding[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "KNN = KNeighborsClassifier()\n",
    "KNN.fit(trainX, trainY)\n",
    "pred = KNN.predict(testX)\n",
    "acc = metrics.accuracy_score(testY, pred)\n",
    "\n",
    "print('TSVD---准确率：%f'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "\n",
    "# dataset = 'pubmed'  #citeseer   cora  pubmed\n",
    "feature = load_data(dataset)[1].toarray()\n",
    "#####逼近式\n",
    "# alpha = 0.99\n",
    "# adj = load_data(dataset)[0]\n",
    "# adj_normalize = normalize_adj(adj)\n",
    "# adj_normalize = np.linalg.inv(np.eye(adj.shape[0]) - 0.9*adj_normalize.toarray())\n",
    "# # adj_normalize = np.eye(adj.shape[0]) + alpha*adj_normalize + alpha**2*np.dot(adj_normalize,adj_normalize.T)\n",
    "\n",
    "embeding = SparseRandomProjection(n_components=dim).fit_transform(feature)\n",
    "\n",
    "trainX , trainY = embeding[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "testX, testY = embeding[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "KNN = KNeighborsClassifier()\n",
    "KNN.fit(trainX, trainY)\n",
    "pred = KNN.predict(testX)\n",
    "acc = metrics.accuracy_score(testY, pred)\n",
    "\n",
    "print('RP---准确率：%f'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "\n",
    "# dataset = 'pubmed'  #citeseer   cora  pubmed\n",
    "feature = load_data(dataset)[1].toarray()\n",
    "#####逼近式\n",
    "# alpha = 0.99\n",
    "# adj = load_data(dataset)[0]\n",
    "# adj_normalize = normalize_adj(adj)\n",
    "# adj_normalize = np.linalg.inv(np.eye(adj.shape[0]) - 0.9*adj_normalize.toarray())\n",
    "# # adj_normalize = np.eye(adj.shape[0]) + alpha*adj_normalize + alpha**2*np.dot(adj_normalize,adj_normalize.T)\n",
    "\n",
    "embeding = LocallyLinearEmbedding(n_neighbors=100,n_components=dim,n_jobs=-1).fit_transform(feature)\n",
    "\n",
    "trainX , trainY = embeding[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "testX, testY = embeding[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "KNN = KNeighborsClassifier()\n",
    "KNN.fit(trainX, trainY)\n",
    "pred = KNN.predict(testX)\n",
    "acc = metrics.accuracy_score(testY, pred)\n",
    "\n",
    "print('LLE---准确率：%f'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "\n",
    "# dataset = 'pubmed'  #citeseer   cora  pubmed\n",
    "feature = load_data(dataset)[1].toarray()\n",
    "#####逼近式\n",
    "# alpha = 0.99\n",
    "# adj = load_data(dataset)[0]\n",
    "# adj_normalize = normalize_adj(adj)\n",
    "# adj_normalize = np.linalg.inv(np.eye(adj.shape[0]) - 0.9*adj_normalize.toarray())\n",
    "# # adj_normalize = np.eye(adj.shape[0]) + alpha*adj_normalize + alpha**2*np.dot(adj_normalize,adj_normalize.T)\n",
    "\n",
    "embeding = TSNE(n_components=2).fit_transform(feature)\n",
    "\n",
    "trainX , trainY = embeding[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "testX, testY = embeding[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "KNN = KNeighborsClassifier()\n",
    "KNN.fit(trainX, trainY)\n",
    "pred = KNN.predict(testX)\n",
    "acc = metrics.accuracy_score(testY, pred)\n",
    "\n",
    "print('t-SNE---准确率：%f'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "\n",
    "# dataset = 'pubmed'  #citeseer   cora  pubmed\n",
    "feature = load_data(dataset)[1].toarray()\n",
    "#####逼近式\n",
    "# alpha = 0.99\n",
    "# adj = load_data(dataset)[0]\n",
    "# adj_normalize = normalize_adj(adj)\n",
    "# adj_normalize = np.linalg.inv(np.eye(adj.shape[0]) - 0.9*adj_normalize.toarray())\n",
    "# # adj_normalize = np.eye(adj.shape[0]) + alpha*adj_normalize + alpha**2*np.dot(adj_normalize,adj_normalize.T)\n",
    "\n",
    "embeding = MDS(n_components=dim).fit_transform(feature)\n",
    "\n",
    "trainX , trainY = embeding[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "testX, testY = embeding[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "KNN = KNeighborsClassifier()\n",
    "KNN.fit(trainX, trainY)\n",
    "pred = KNN.predict(testX)\n",
    "acc = metrics.accuracy_score(testY, pred)\n",
    "\n",
    "print('MDS---准确率：%f'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "\n",
    "# dataset = 'pubmed'  #citeseer   cora  pubmed\n",
    "feature = load_data(dataset)[1].toarray()\n",
    "#####逼近式\n",
    "# alpha = 0.99\n",
    "# adj = load_data(dataset)[0]\n",
    "# adj_normalize = normalize_adj(adj)\n",
    "# adj_normalize = np.linalg.inv(np.eye(adj.shape[0]) - 0.9*adj_normalize.toarray())\n",
    "# # adj_normalize = np.eye(adj.shape[0]) + alpha*adj_normalize + alpha**2*np.dot(adj_normalize,adj_normalize.T)\n",
    "\n",
    "embeding = Isomap(n_components=dim,n_neighbors=100,n_jobs=-1).fit_transform(feature)\n",
    "\n",
    "trainX , trainY = embeding[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "testX, testY = embeding[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "KNN = KNeighborsClassifier()\n",
    "KNN.fit(trainX, trainY)\n",
    "pred = KNN.predict(testX)\n",
    "acc = metrics.accuracy_score(testY, pred)\n",
    "\n",
    "print('Isomap---准确率：%f'%acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "\n",
    "# dataset = 'pubmed'  #citeseer   cora  pubmed\n",
    "feature = load_data(dataset)[1].toarray()\n",
    "#####逼近式\n",
    "# alpha = 0.99\n",
    "# adj = load_data(dataset)[0]\n",
    "# adj_normalize = normalize_adj(adj)\n",
    "# adj_normalize = np.linalg.inv(np.eye(adj.shape[0]) - 0.9*adj_normalize.toarray())\n",
    "# # adj_normalize = np.eye(adj.shape[0]) + alpha*adj_normalize + alpha**2*np.dot(adj_normalize,adj_normalize.T)\n",
    "\n",
    "embeding = KernelPCA(n_components=dim,kernel='cosine').fit_transform(feature)\n",
    "\n",
    "trainX , trainY = embeding[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "testX, testY = embeding[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "KNN = KNeighborsClassifier()\n",
    "KNN.fit(trainX, trainY)\n",
    "pred = KNN.predict(testX)\n",
    "acc = metrics.accuracy_score(testY, pred)\n",
    "\n",
    "print('KPCA---准确率：%f'%acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense  ,Dot ,Lambda, Dropout, BatchNormalization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model  \n",
    "from keras.datasets import mnist  \n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "import keras\n",
    "import numpy as np  \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt  \n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "K.clear_session()\n",
    "# 指定第一块GPU可用 \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)\n",
    "\n",
    "\n",
    "\n",
    "dataset = 'citeseer'  #citeseer   cora  pubmed\n",
    "\n",
    "feature = load_data(dataset)[1].toarray()\n",
    "\n",
    "y_train = load_data(dataset)[2][load_data(dataset)[5]].tolist()\n",
    "\n",
    "adj = load_data(dataset)[0]\n",
    "D = np.sum(adj.toarray(),axis=0,dtype='float32')\n",
    "alpha = np.diag(np.power(D,-1))\n",
    "adj_normalize = normalize_adj(adj)\n",
    "adj_normalize = np.linalg.inv(np.eye(adj.shape[0]) - 0.85*np.dot(alpha,adj.toarray()))\n",
    "# adj_normalize = np.eye(adj.shape[0]) + alpha*adj_normalize + alpha**2*np.dot(adj_normalize,adj_normalize.T)\n",
    "adj_normalize_tf = tf.convert_to_tensor(adj_normalize,dtype='float32')\n",
    "\n",
    "encoding_dim_1 = 2048\n",
    "encoding_dim_2 = 512\n",
    "\n",
    "order_0 = Input(shape=(feature.shape[1],)) \n",
    "\n",
    "def graph_convex(x):\n",
    "    x1 = K.dot(adj_normalize_tf,x)\n",
    "    return x1\n",
    "\n",
    "###########不对称\n",
    "order_1 = Lambda(function=graph_convex)(order_0)\n",
    "encoded_1 = BatchNormalization(axis = -1)(order_1)\n",
    "encoded_1 = Dense(encoding_dim_1, activation='tanh', use_bias=True)(encoded_1)  \n",
    "# encoded_1 = Dropout(0.5)(encoded_1)\n",
    "# encoded_1 = BatchNormalization(axis = -1)(encoded_1)\n",
    "\n",
    "order_2 = Lambda(function= graph_convex)(encoded_1)\n",
    "encoded_2 = BatchNormalization(axis = -1)(order_2)\n",
    "encoded_2 = Dense(encoding_dim_2, activation='sigmoid', use_bias=True)(encoded_2)  \n",
    "# encoded_2 = Dropout(0.5)(encoded_2)\n",
    "# encoded_22 = BatchNormalization(axis = -1)(encoded_2)\n",
    "\n",
    "order_3 = Lambda(function= graph_convex)(encoded_2)\n",
    "decoded_1 = BatchNormalization(axis = -1)(order_3)\n",
    "decoded_1 = Dense(encoding_dim_1, activation='tanh', use_bias=True)(decoded_1)  \n",
    "# decoded_1 = Dropout(0.5)(decoded_1)\n",
    "# decoded_1 = BatchNormalization(axis = -1)(decoded_1)\n",
    "\n",
    "order_4 = Lambda(function= graph_convex)(decoded_1)\n",
    "decoded_2 = BatchNormalization(axis = -1)(order_4)\n",
    "decoded_2 = Dense(feature.shape[1], activation='relu', use_bias=True)(decoded_2)  \n",
    "#     decoded_2 = BatchNormalization(axis = -1)(decoded_2)\n",
    "\n",
    "\n",
    "autoencoder = Model(inputs=order_0, outputs=decoded_2)  \n",
    "encoder_1 = Model(inputs=order_0, outputs=encoded_1)    \n",
    "encoder_2 = Model(inputs=order_0, outputs=encoded_2) \n",
    "decoder_1 = Model(inputs=order_0, outputs=decoded_1)\n",
    "\n",
    "def myloss(y_true, y_pred, e= 0.1):\n",
    "#     recon = K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "    recon = K.mean(K.square(y_true - y_pred), axis=-1)\n",
    "    sparse = -e*K.log(K.mean(encoded_2,axis=-1)/e) - (1-e)*K.log((1-K.mean(encoded_2,axis=-1))/(1-e))\n",
    "    #         orth = K.mean(K.square(K.dot(K.dot(K.transpose(encoded_2), D), encoded_2) - K.eye(encoding_dim_2)))\n",
    "    return recon + 0*sparse\n",
    "\n",
    "#####结构保持loss + 重构loss############\n",
    "# autoencoder.compile(optimizer='Adam', loss=lambda y_true,y_pred:y_pred)\n",
    "########重构loss#################\n",
    "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "autoencoder.compile(optimizer='adam', loss=myloss)\n",
    "autoencoder.fit(feature, feature, epochs=50,batch_size=feature.shape[0],shuffle=False, verbose=1)\n",
    "##cora: 50  citeseer:   pubmed: \n",
    "\n",
    "########三层########\n",
    "# encoder_2.compile(optimizer=adam , loss='binary_crossentropy')\n",
    "# encoder_2.fit(feature, feature, epochs=300,batch_size=feature.shape[0],shuffle=False)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "\n",
    "trainX , trainY = encoder_2.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "testX, testY = encoder_2.predict(feature,batch_size=feature.shape[0])[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(trainX,trainY)\n",
    "pred_KNN = knn.predict(testX)\n",
    "auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "print('KNN_auc:%f'%auc_KNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse.linalg as slin\n",
    "\n",
    "\n",
    "def eigen_reweighting(X, order, coef):\n",
    "#X: original eigenvalues\n",
    "# order: order, -1 stands for infinity\n",
    "# coef: weights, decaying constant if order = -1\n",
    "    if order==-1:\n",
    "        if len(coef)==1:\n",
    "            if np.max(np.abs(X))*coef[0]<1:\n",
    "                X_h = X/(1-coef[0]*X)\n",
    "            else:\n",
    "                print('Decaying constant is too large')\n",
    "        else:\n",
    "            print('Eigen reweighting wrong')\n",
    "    elif len(coef)==order:\n",
    "        X_h = coef[0]*X\n",
    "        X_temp = X\n",
    "        for i in range(1, order):\n",
    "            #print('!')\n",
    "            X_temp = X_temp*X\n",
    "            X_h = X_h +coef[i]*X_temp\n",
    "    else:\n",
    "        print('Eigen reweighting wrong')\n",
    "    return X_h    \n",
    "\n",
    "\n",
    "def eigen_topL(A, d):\n",
    "# A: N x N symmetric sparse adjacency matrix\n",
    "# d: present dimension\n",
    "# return top-L eigen-decomposition of A containing at least d positive eigenvalues\n",
    "    if not np.allclose(A, np.transpose(A), atol=1e-8):\n",
    "        print('The matrix is not symmetric!')\n",
    "    L = d+10\n",
    "    while 1:\n",
    "        L+=d\n",
    "        l, x = slin.eigs(A, k = L)\n",
    "        if np.sum(l>0)>=d:\n",
    "            break\n",
    "    #select only top k\n",
    "    inds = np.argsort(-np.absolute(l))\n",
    "    max_ind = np.where(np.cumsum(l>0)>=d)\n",
    "    l = l[:max_ind[0][0]]\n",
    "    inds = inds[:max_ind[0][0]]\n",
    "    x = x[:, inds]\n",
    "    return l, x\n",
    "\n",
    "\n",
    "def shift_embedding(lmbd, X, order, coef, d):\n",
    "# lambda,X: top-L eigen-decomposition \n",
    "# order: a number indicating the order\n",
    "# coef: a vector of length order, indicating the weights for each order\n",
    "# d: preset embedding dimension\n",
    "# return: content/context embedding vectors \n",
    "    lambda_h = eigen_reweighting(lmbd, order, coef)\n",
    "    temp_index = np.argsort(-np.absolute(lambda_h))\n",
    "    temp_index = temp_index[:d]\n",
    "    lambda_h = lambda_h[temp_index]\n",
    "    U = np.dot(X[:, temp_index], np.diag(np.sqrt(np.absolute(lambda_h))))\n",
    "    V = np.dot(X[:, temp_index],  np.diag(np.sqrt(np.absolute(lambda_h))))*np.sign(lambda_h)\n",
    "\n",
    "    return U, V\n",
    "\n",
    "\n",
    "def AROPE(A, d, order, weights):\n",
    "# AROPE Algortihm\n",
    "# Inputs: \n",
    "# A: adjacency matrix A or its variations\n",
    "# d: dimensionality \n",
    "# r different high-order proximity:\n",
    "    # order: 1 x r vector, order of the proximity\n",
    "    # weights: dictionary of r elements, each containing the weights for one high-order proximity\n",
    "# Outputs: List of r elements, each containing the embedding Matrices     \n",
    "    lambd, X = eigen_topL(A, d)\n",
    "    r = len(order)\n",
    "    U = []\n",
    "    V = []\n",
    "    for i in range(r):\n",
    "        Ui, Vi = shift_embedding(lambd, X, order[i], weights[i], d)\n",
    "        U.append(Ui)\n",
    "        V.append(Vi)\n",
    "    return U, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = load_data('cora')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = adj.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = AROPE(adj,512,[3],[[1,0.9,0.81]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U , V = AROPE(adj,512,[1],[[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = U[0].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "U =sio.loadmat('pubmed_U.mat')['U'][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = V[0].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "def onehot2index(onehot):\n",
    "    index = []\n",
    "    onehot = onehot.tolist()\n",
    "    for i in onehot:\n",
    "        index.append(i.index(1))\n",
    "    return np.array(index)\n",
    "dataset = 'pubmed'\n",
    "# SDNE = sio.loadmat(\"/home/lifuzhen/SDNE/result/blogCatalog-Mon-Oct-15-16:22:54-2018/embedding.mat\")['embedding']\n",
    "trainX , trainY = U[load_data(dataset)[5]] , onehot2index(load_data(dataset)[2][load_data(dataset)[5]])\n",
    "testX, testY = U[load_data(dataset)[7]] , onehot2index(load_data(dataset)[4][load_data(dataset)[7]])\n",
    "\n",
    "SVC = SVC(C=0.5,kernel='rbf')\n",
    "GBDT = GradientBoostingClassifier(n_estimators=8)\n",
    "RF = RandomForestClassifier()\n",
    "KNN = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "SVC.fit(trainX, trainY)\n",
    "GBDT.fit(trainX, trainY)\n",
    "RF.fit(trainX, trainY)\n",
    "KNN.fit(trainX,trainY)\n",
    "\n",
    "pred_SVC = SVC.predict(testX)\n",
    "pred_GBDT = GBDT.predict(testX)\n",
    "pred_RF = RF.predict(testX)\n",
    "pred_KNN = KNN.predict(testX)\n",
    "\n",
    "auc_SVC = metrics.accuracy_score(testY , pred_SVC)\n",
    "auc_GBDT = metrics.accuracy_score(testY , pred_GBDT)\n",
    "auc_RF = metrics.accuracy_score(testY , pred_RF)\n",
    "auc_KNN = metrics.accuracy_score(testY , pred_KNN)\n",
    "\n",
    "print('SVC_auc：%f'%auc_SVC)\n",
    "print('GBDT_auc：%f'%auc_GBDT)\n",
    "print('RF_auc：%f'%auc_RF)\n",
    "print('KNN_auc:%f'%auc_KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = load_data('pubmed')[0].toarray()\n",
    "sio.savemat('/home/lifuzhen/pubmed.mat',{'array':adj})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
